{   "model":"yahma/llama-7b-hf",
    "dataset": "alpaca",
    "lora_rank": 4,
    "learning_rate": 3e-4,
    "lora_dropout": 0.05,
    "weight_decay": 0.00,
    "n_skills": 8,       
    "load_in_8bit":0,  
    "model_family":"gpt",   

    "predict_batch_size": 4,
    "micro_batch_size": 4,  
    "train_batch_size": 32,
    "max_input_length": 256,    

    "model_modifier": "vsmear_wreg",
    "modify_modules": ".*attn.*",       
    "modify_layers": "q_proj|v_proj|k_proj",   
    "trainable_param_names": ".*lora_[ab].*|.*selector.*",
        
    "eval_hellaswag": 0,
    "eval_arc": 0, 
    "eval_truthfulqa": 0,

    "wandb_project": "test"

}