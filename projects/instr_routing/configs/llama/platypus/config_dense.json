{
    "dataset": "platypus",
    "fast_dev_run": 0,
    "optimizer": "adamw", 
    "learning_rate": 3e-4, 
    "micro_batch_size": 1,        
    "wandb_project": "alpaca_tuning", 
    "exp_name": "platypus_dense",
    "precision": "bf16",  
    "xrouter_normal_innit": 1,
    "train_batch_size": 16,
    "merge_A_B_seperately": 0,
    "model": "yahma/llama-7b-hf",
    "lora_rank": 4,  
    "rank": 4,
    "num_train_epochs": 1,
    "n_skills": 1,
    "share_lora_a": 0,
    "load_in_8bit": 1,  
    "router_selector_cluster_temp": 0.1,
    "max_input_length": 4096,
    "eval_every": 400,
    "warmup_steps": 100,    
    "same_lora_init": 0,
    "lora_dropout": 0.05,
    "weight_decay": 0.00,
    "router_selector": "poly",    
    "model_modifier": "poly_lora",  
    "lora_modules": ".*mlp",  
    "lora_layers": "gate_proj|down_proj|up_proj",
    "trainable_param_names": ".*lora_[ab].*"
}
