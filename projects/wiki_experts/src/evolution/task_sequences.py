# original sampling process
# created all possible pairs, ordered them, then sampled 15 pairs linarly over the ordered list
# added 1 pair manually "anatomy, professional_medicine"
# 16 pairs formed originally, came from 31 tasks, see ADAUNI_INTERFERENCE_31
# these opriginal pairs
similarity_31 = {
    "wiki_hop_original_choose_best_object_affirmative_1, wiki_hop_original_choose_best_object_affirmative_2": 0.9884,
    "anatomy, professional_medicine": 0.966,
    "cot_strategyqa, airoboros": 0.734,
    "quac_1_0_0, high_school_government_and_politics": 0.6814,
    "math_dataset_algebra__linear_1d_1_0_0, niv2_grammar_error_detection": 0.6477,
    "niv2_sentence_composition, wiki_bio_guess_person": 0.6225,
    "web_questions_whats_the_answer, ropes_background_situation_middle": 0.6007,
    "sciq_Direct_Question, ropes_new_situation_background_answer": 0.5815,
    "human_aging, cot_ecqa_ii": 0.5625,
    "cot_esnli, airoboros": 0.5436,
    "niv2_discourse_connective_identification, sociology": 0.5243,
    "niv2_code_to_text, coqa_1_0_0": 0.5029,
    "super_glue_wic_1_0_2, college_medicine": 0.4783,
    "ropes_prompt_bottom_no_hint, fix_punct": 0.4482,
    "niv2_ethics_classification, guanaco": 0.4005,
    "wiqa_what_might_be_the_first_step_of_the_process, ultrachat_19": 0.1411,
}
# the resulting tasks
ADAUNI_INTERFERENCE_31 = [
    "wiki_hop_original_choose_best_object_affirmative_1",
    "wiki_hop_original_choose_best_object_affirmative_2",
    "anatomy",
    "professional_medicine",
    "cot_strategyqa",
    "airoboros",
    "quac_1_0_0",
    "high_school_government_and_politics",
    "math_dataset_algebra__linear_1d_1_0_0",
    "niv2_grammar_error_detection",
    "niv2_sentence_composition",
    "wiki_bio_guess_person",
    "web_questions_whats_the_answer",
    "ropes_background_situation_middle",
    "sciq_Direct_Question",
    "ropes_new_situation_background_answer",
    "human_aging",
    "cot_ecqa_ii",
    "cot_esnli",
    "niv2_discourse_connective_identification",
    "sociology",
    "niv2_code_to_text",
    "coqa_1_0_0",
    "super_glue_wic_1_0_2",
    "college_medicine",
    "ropes_prompt_bottom_no_hint",
    "fix_punct",
    "niv2_ethics_classification",
    "guanaco",
    "wiqa_what_might_be_the_first_step_of_the_process",
    "ultrachat_19",
]

# filtered pairs only to contains tasks with 10k samples -> 11 pairs
similarity_31_10k = {
    "wiki_hop_original_choose_best_object_affirmative_1, wiki_hop_original_choose_best_object_affirmative_2": 0.9884,
    "anatomy, professional_medicine": 0.966,
    "quac_1_0_0, high_school_government_and_politics": 0.6814,
    "math_dataset_algebra__linear_1d_1_0_0, niv2_grammar_error_detection": 0.6477,
    "niv2_sentence_composition, wiki_bio_guess_person": 0.6225,
    "web_questions_whats_the_answer, ropes_background_situation_middle": 0.6007,
    "sciq_Direct_Question, ropes_new_situation_background_answer": 0.5815,
    "niv2_code_to_text, coqa_1_0_0": 0.5029,
    "super_glue_wic_1_0_2, college_medicine": 0.4783,
    "ropes_prompt_bottom_no_hint, fix_punct": 0.4482,
    "wiqa_what_might_be_the_first_step_of_the_process, ultrachat_19": 0.1411,
}

# sampled additional pairs from the same set of 31 tasks to get 30 pairs
similarity_31_10k_extended = {
    "wiki_hop_original_choose_best_object_affirmative_1, wiki_hop_original_choose_best_object_affirmative_2": 0.9884,
    "anatomy, professional_medicine": 0.966,
    "quac_1_0_0, high_school_government_and_politics": 0.6814,
    "math_dataset_algebra__linear_1d_1_0_0, niv2_grammar_error_detection": 0.6477,
    "niv2_sentence_composition, wiki_bio_guess_person": 0.6225,
    "web_questions_whats_the_answer, ropes_background_situation_middle": 0.6007,
    "sciq_Direct_Question, ropes_new_situation_background_answer": 0.5815,
    "niv2_code_to_text, coqa_1_0_0": 0.5029,
    "super_glue_wic_1_0_2, college_medicine": 0.4783,
    "ropes_prompt_bottom_no_hint, fix_punct": 0.4482,
    "wiqa_what_might_be_the_first_step_of_the_process, ultrachat_19": 0.1411,
    "anatomy, ultrachat_19": 0.1949,
    "ultrachat_19, ropes_new_situation_background_answer": 0.1826,
    "ultrachat_19, college_medicine": 0.1985,
    "ultrachat_19, wiki_hop_original_choose_best_object_affirmative_1": 0.1806,
    "ropes_prompt_bottom_no_hint, ultrachat_19": 0.1997,
    "wiqa_what_might_be_the_first_step_of_the_process, coqa_1_0_0": 0.3035,
    "ultrachat_19, niv2_sentence_composition": 0.2877,
    "fix_punct, wiki_hop_original_choose_best_object_affirmative_1": 0.3928,
    "human_aging, niv2_grammar_error_detection": 0.3743,
    "web_questions_whats_the_answer, ultrachat_19": 0.212,
    "wiqa_what_might_be_the_first_step_of_the_process, niv2_sentence_composition": 0.3977,
    "ropes_prompt_bottom_no_hint, wiki_hop_original_choose_best_object_affirmative_2": 0.5141,
    "cot_esnli, human_aging": 0.5444,
    "niv2_ethics_classification, niv2_sentence_composition": 0.6176,
    "niv2_ethics_classification, web_questions_whats_the_answer": 0.6213,
    "super_glue_wic_1_0_2, sciq_Direct_Question": 0.8107,
    "college_medicine, human_aging": 0.9039,
    "math_dataset_algebra__linear_1d_1_0_0, sciq_Direct_Question": 0.8134,
    "super_glue_wic_1_0_2, math_dataset_algebra__linear_1d_1_0_0": 0.823,
}

# similarity pairs
# selected from full library sordonia/library-phi_2-v2 using SVD embeddings
# Pair: wiki_hop_original_choose_best_object_affirmative_1, wiki_hop_original_choose_best_object_affirmative_2, similarity: 0.9884
# Pair: anatomy,professional_medicine, similarity: 0.966
# Pair: cot_strategyqa, airoboros, similarity: 0.734
# Pair: quac_1_0_0, high_school_government_and_politics, similarity: 0.6814
# Pair: math_dataset_algebra__linear_1d_1_0_0, niv2_grammar_error_detection, similarity: 0.6477
# Pair: niv2_sentence_composition, wiki_bio_guess_person, similarity: 0.6225
# Pair: web_questions_whats_the_answer, ropes_background_situation_middle, similarity: 0.6007
# Pair: sciq_Direct_Question, ropes_new_situation_background_answer, similarity: 0.5815
# Pair: human_aging, cot_ecqa_ii, similarity: 0.5625
# Pair: cot_esnli, airoboros, similarity: 0.5436
# Pair: niv2_discourse_connective_identification, sociology, similarity: 0.5243
# Pair: niv2_code_to_text, coqa_1_0_0, similarity: 0.5029
# Pair: super_glue_wic_1_0_2, college_medicine, similarity: 0.4783
# Pair: ropes_prompt_bottom_no_hint, fix_punct, similarity: 0.4482
# Pair: niv2_ethics_classification, guanaco, similarity: 0.4005
# Pair: wiqa_what_might_be_the_first_step_of_the_process, ultrachat_19, similarity: 0.1411

# all pairs that can be samples from the 31 tasks above (ADAUNI_INTERFERENCE_31)
similarity_full_31 = {
    "anatomy, college_medicine": 0.9753,
    "college_medicine, sociology": 0.9687,
    "anatomy, professional_medicine": 0.9646,
    "college_medicine, high_school_government_and_politics": 0.9632,
    "professional_medicine, high_school_government_and_politics": 0.9588,
    "anatomy, high_school_government_and_politics": 0.9585,
    "professional_medicine, college_medicine": 0.9518,
    "high_school_government_and_politics, sociology": 0.9459,
    "anatomy, sociology": 0.9398,
    "anatomy, human_aging": 0.9263,
    "professional_medicine, sociology": 0.9169,
    "college_medicine, human_aging": 0.9039,
    "human_aging, sociology": 0.8932,
    "professional_medicine, human_aging": 0.8905,
    "human_aging, high_school_government_and_politics": 0.8861,
    "ropes_new_situation_background_answer, ropes_background_situation_middle": 0.8704,
    "niv2_ethics_classification, niv2_discourse_connective_identification": 0.831,
    "super_glue_wic_1_0_2, math_dataset_algebra__linear_1d_1_0_0": 0.823,
    "math_dataset_algebra__linear_1d_1_0_0, sciq_Direct_Question": 0.8134,
    "super_glue_wic_1_0_2, sciq_Direct_Question": 0.8107,
    "math_dataset_algebra__linear_1d_1_0_0, niv2_code_to_text": 0.8079,
    "airoboros, guanaco": 0.7872,
    "ropes_prompt_bottom_no_hint, ropes_background_situation_middle": 0.7743,
    "niv2_sentence_composition, airoboros": 0.766,
    "sciq_Direct_Question, niv2_code_to_text": 0.7659,
    "wiki_hop_original_choose_best_object_affirmative_2, coqa_1_0_0": 0.7472,
    "wiki_hop_original_choose_best_object_affirmative_1, coqa_1_0_0": 0.7445,
    "niv2_ethics_classification, math_dataset_algebra__linear_1d_1_0_0": 0.7436,
    "airoboros, niv2_discourse_connective_identification": 0.7346,
    "ropes_prompt_bottom_no_hint, ropes_new_situation_background_answer": 0.7343,
    "math_dataset_algebra__linear_1d_1_0_0, niv2_discourse_connective_identification": 0.7334,
    "niv2_sentence_composition, niv2_discourse_connective_identification": 0.7212,
    "niv2_grammar_error_detection, niv2_discourse_connective_identification": 0.7134,
    "sciq_Direct_Question, niv2_discourse_connective_identification": 0.7099,
    "anatomy, niv2_sentence_composition": 0.708,
    "cot_ecqa_ii, niv2_discourse_connective_identification": 0.7036,
    "cot_strategyqa, ropes_background_situation_middle": 0.7031,
    "high_school_government_and_politics, niv2_sentence_composition": 0.6986,
    "quac_1_0_0, airoboros": 0.6971,
    "super_glue_wic_1_0_2, niv2_code_to_text": 0.6929,
    "niv2_code_to_text, niv2_discourse_connective_identification": 0.6867,
    "niv2_ethics_classification, cot_ecqa_ii": 0.6859,
    "ropes_prompt_bottom_no_hint, quac_1_0_0": 0.6852,
    "college_medicine, niv2_sentence_composition": 0.6827,
    "super_glue_wic_1_0_2, niv2_discourse_connective_identification": 0.6822,
    "niv2_ethics_classification, sciq_Direct_Question": 0.6813,
    "quac_1_0_0, ropes_new_situation_background_answer": 0.6795,
    "professional_medicine, niv2_sentence_composition": 0.679,
    "math_dataset_algebra__linear_1d_1_0_0, fix_punct": 0.6771,
    "cot_strategyqa, niv2_sentence_composition": 0.6766,
    "quac_1_0_0, sociology": 0.6756,
    "math_dataset_algebra__linear_1d_1_0_0, airoboros": 0.6748,
    "human_aging, niv2_sentence_composition": 0.6747,
    "anatomy, ropes_prompt_bottom_no_hint": 0.6744,
    "sciq_Direct_Question, cot_ecqa_ii": 0.6735,
    "niv2_ethics_classification, niv2_grammar_error_detection": 0.6721,
    "web_questions_whats_the_answer, cot_ecqa_ii": 0.6721,
    "niv2_sentence_composition, sociology": 0.6699,
    "wiki_bio_guess_person, ropes_background_situation_middle": 0.6686,
    "college_medicine, quac_1_0_0": 0.668,
    "sciq_Direct_Question, wiki_bio_guess_person": 0.6677,
    "ropes_prompt_bottom_no_hint, high_school_government_and_politics": 0.6621,
    "airoboros, coqa_1_0_0": 0.6598,
    "cot_ecqa_ii, niv2_sentence_composition": 0.6596,
    "math_dataset_algebra__linear_1d_1_0_0, coqa_1_0_0": 0.6578,
    "web_questions_whats_the_answer, niv2_sentence_composition": 0.6566,
    "professional_medicine, cot_strategyqa": 0.6546,
    "niv2_ethics_classification, cot_strategyqa": 0.6545,
    "human_aging, cot_strategyqa": 0.6532,
    "sciq_Direct_Question, fix_punct": 0.6524,
    "quac_1_0_0, cot_strategyqa": 0.6507,
    "cot_strategyqa, coqa_1_0_0": 0.6507,
    "ropes_new_situation_background_answer, cot_strategyqa": 0.6491,
    "niv2_sentence_composition, ropes_background_situation_middle": 0.6491,
    "anatomy, quac_1_0_0": 0.6476,
    "cot_strategyqa, sociology": 0.6461,
    "cot_ecqa_ii, airoboros": 0.6457,
    "anatomy, cot_strategyqa": 0.6451,
    "ropes_prompt_bottom_no_hint, college_medicine": 0.6442,
    "high_school_government_and_politics, airoboros": 0.6439,
    "anatomy, airoboros": 0.6436,
    "college_medicine, airoboros": 0.6436,
    "cot_strategyqa, cot_ecqa_ii": 0.6434,
    "cot_strategyqa, high_school_government_and_politics": 0.6429,
    "ropes_new_situation_background_answer, airoboros": 0.6415,
    "math_dataset_algebra__linear_1d_1_0_0, cot_ecqa_ii": 0.6402,
    "ropes_prompt_bottom_no_hint, professional_medicine": 0.64,
    "web_questions_whats_the_answer, cot_strategyqa": 0.6382,
    "sciq_Direct_Question, airoboros": 0.6374,
    "professional_medicine, airoboros": 0.6372,
    "wiki_bio_guess_person, airoboros": 0.6332,
    "airoboros, sociology": 0.6331,
    "airoboros, ropes_background_situation_middle": 0.6331,
    "ropes_prompt_bottom_no_hint, human_aging": 0.6326,
    "college_medicine, cot_strategyqa": 0.6314,
    "anatomy, ropes_background_situation_middle": 0.6305,
    "quac_1_0_0, coqa_1_0_0": 0.6305,
    "math_dataset_algebra__linear_1d_1_0_0, quac_1_0_0": 0.6288,
    "niv2_ethics_classification, niv2_code_to_text": 0.6283,
    "math_dataset_algebra__linear_1d_1_0_0, cot_strategyqa": 0.6271,
    "wiki_bio_guess_person, niv2_discourse_connective_identification": 0.6262,
    "cot_esnli, cot_ecqa_ii": 0.6252,
    "human_aging, ropes_background_situation_middle": 0.6223,
    "web_questions_whats_the_answer, math_dataset_algebra__linear_1d_1_0_0": 0.622,
    "sciq_Direct_Question, quac_1_0_0": 0.6215,
    "fix_punct, ropes_background_situation_middle": 0.6215,
    "niv2_ethics_classification, web_questions_whats_the_answer": 0.6213,
    "math_dataset_algebra__linear_1d_1_0_0, ropes_background_situation_middle": 0.6212,
    "web_questions_whats_the_answer, fix_punct": 0.6209,
    "math_dataset_algebra__linear_1d_1_0_0, guanaco": 0.6206,
    "cot_esnli, cot_strategyqa": 0.6205,
    "niv2_sentence_composition, coqa_1_0_0": 0.62,
    "professional_medicine, quac_1_0_0": 0.6197,
    "ropes_new_situation_background_answer, wiki_hop_original_choose_best_object_affirmative_1": 0.6189,
    "sciq_Direct_Question, cot_strategyqa": 0.6182,
    "super_glue_wic_1_0_2, airoboros": 0.618,
    "niv2_ethics_classification, niv2_sentence_composition": 0.6176,
    "super_glue_wic_1_0_2, guanaco": 0.6157,
    "math_dataset_algebra__linear_1d_1_0_0, ropes_new_situation_background_answer": 0.6156,
    "web_questions_whats_the_answer, sciq_Direct_Question": 0.6146,
    "niv2_code_to_text, airoboros": 0.6137,
    "math_dataset_algebra__linear_1d_1_0_0, wiki_bio_guess_person": 0.6126,
    "ropes_new_situation_background_answer, wiki_hop_original_choose_best_object_affirmative_2": 0.6115,
    "ropes_new_situation_background_answer, wiki_bio_guess_person": 0.6092,
    "ropes_new_situation_background_answer, coqa_1_0_0": 0.609,
    "web_questions_whats_the_answer, airoboros": 0.6075,
    "cot_esnli, coqa_1_0_0": 0.6072,
    "ropes_prompt_bottom_no_hint, sociology": 0.6067,
    "niv2_ethics_classification, ropes_background_situation_middle": 0.6055,
    "guanaco, niv2_discourse_connective_identification": 0.605,
    "niv2_ethics_classification, fix_punct": 0.6037,
    "quac_1_0_0, wiki_hop_original_choose_best_object_affirmative_1": 0.6018,
    "web_questions_whats_the_answer, niv2_discourse_connective_identification": 0.6009,
    "web_questions_whats_the_answer, human_aging": 0.6,
    "cot_strategyqa, fix_punct": 0.5991,
    "niv2_ethics_classification, coqa_1_0_0": 0.599,
    "niv2_ethics_classification, super_glue_wic_1_0_2": 0.5989,
    "niv2_ethics_classification, airoboros": 0.5975,
    "cot_esnli, sciq_Direct_Question": 0.5974,
    "math_dataset_algebra__linear_1d_1_0_0, wiki_hop_original_choose_best_object_affirmative_1": 0.5969,
    "wiki_bio_guess_person, niv2_grammar_error_detection": 0.5962,
    "sciq_Direct_Question, niv2_sentence_composition": 0.5934,
    "cot_esnli, niv2_sentence_composition": 0.5925,
    "quac_1_0_0, ropes_background_situation_middle": 0.5924,
    "cot_esnli, high_school_government_and_politics": 0.5921,
    "high_school_government_and_politics, ropes_background_situation_middle": 0.5911,
    "ropes_prompt_bottom_no_hint, niv2_sentence_composition": 0.5894,
    "niv2_sentence_composition, niv2_grammar_error_detection": 0.5891,
    "wiqa_what_might_be_the_first_step_of_the_process, math_dataset_algebra__linear_1d_1_0_0": 0.5889,
    "cot_esnli, sociology": 0.588,
    "sciq_Direct_Question, ropes_background_situation_middle": 0.5878,
    "web_questions_whats_the_answer, high_school_government_and_politics": 0.5877,
    "wiki_hop_original_choose_best_object_affirmative_1, ropes_background_situation_middle": 0.5876,
    "ropes_prompt_bottom_no_hint, cot_strategyqa": 0.587,
    "anatomy, web_questions_whats_the_answer": 0.5866,
    "professional_medicine, ropes_background_situation_middle": 0.5845,
    "wiki_hop_original_choose_best_object_affirmative_1, airoboros": 0.5845,
    "wiqa_what_might_be_the_first_step_of_the_process, sciq_Direct_Question": 0.5843,
    "super_glue_wic_1_0_2, cot_ecqa_ii": 0.5833,
    "professional_medicine, guanaco": 0.5829,
    "quac_1_0_0, niv2_discourse_connective_identification": 0.5824,
    "cot_strategyqa, wiki_hop_original_choose_best_object_affirmative_1": 0.5804,
    "college_medicine, guanaco": 0.5794,
    "niv2_sentence_composition, guanaco": 0.5792,
    "wiki_hop_original_choose_best_object_affirmative_1, niv2_sentence_composition": 0.5791,
    "college_medicine, ropes_background_situation_middle": 0.5789,
    "college_medicine, coqa_1_0_0": 0.5778,
    "cot_ecqa_ii, high_school_government_and_politics": 0.5776,
    "super_glue_wic_1_0_2, ropes_new_situation_background_answer": 0.5764,
    "college_medicine, wiki_hop_original_choose_best_object_affirmative_1": 0.5755,
    "cot_esnli, college_medicine": 0.5749,
    "anatomy, cot_esnli": 0.5735,
    "human_aging, quac_1_0_0": 0.5734,
    "fix_punct, niv2_grammar_error_detection": 0.5721,
    "quac_1_0_0, wiki_hop_original_choose_best_object_affirmative_2": 0.5718,
    "web_questions_whats_the_answer, college_medicine": 0.5717,
    "web_questions_whats_the_answer, sociology": 0.571,
    "niv2_discourse_connective_identification, ropes_background_situation_middle": 0.5692,
    "fix_punct, niv2_code_to_text": 0.569,
    "cot_esnli, math_dataset_algebra__linear_1d_1_0_0": 0.5687,
    "sciq_Direct_Question, coqa_1_0_0": 0.5682,
    "high_school_government_and_politics, guanaco": 0.5682,
    "sociology, coqa_1_0_0": 0.568,
    "quac_1_0_0, guanaco": 0.5674,
    "super_glue_wic_1_0_2, fix_punct": 0.5673,
    "cot_strategyqa, niv2_discourse_connective_identification": 0.5673,
    "wiki_bio_guess_person, niv2_code_to_text": 0.5669,
    "wiki_hop_original_choose_best_object_affirmative_1, high_school_government_and_politics": 0.5666,
    "math_dataset_algebra__linear_1d_1_0_0, wiki_hop_original_choose_best_object_affirmative_2": 0.5652,
    "sciq_Direct_Question, high_school_government_and_politics": 0.564,
    "guanaco, coqa_1_0_0": 0.5621,
    "niv2_code_to_text, guanaco": 0.5618,
    "niv2_ethics_classification, wiki_hop_original_choose_best_object_affirmative_1": 0.5617,
    "quac_1_0_0, wiki_bio_guess_person": 0.561,
    "super_glue_wic_1_0_2, niv2_grammar_error_detection": 0.5604,
    "cot_ecqa_ii, fix_punct": 0.5601,
    "wiki_hop_original_choose_best_object_affirmative_2, ropes_background_situation_middle": 0.5599,
    "niv2_ethics_classification, cot_esnli": 0.5597,
    "anatomy, wiki_hop_original_choose_best_object_affirmative_1": 0.5597,
    "cot_ecqa_ii, sociology": 0.5592,
    "airoboros, wiki_hop_original_choose_best_object_affirmative_2": 0.5592,
    "high_school_government_and_politics, niv2_discourse_connective_identification": 0.5586,
    "college_medicine, niv2_discourse_connective_identification": 0.5583,
    "anatomy, niv2_discourse_connective_identification": 0.5578,
    "wiki_hop_original_choose_best_object_affirmative_1, niv2_discourse_connective_identification": 0.5572,
    "cot_esnli, super_glue_wic_1_0_2": 0.557,
    "wiki_hop_original_choose_best_object_affirmative_1, sociology": 0.5567,
    "sociology, ropes_background_situation_middle": 0.5564,
    "super_glue_wic_1_0_2, quac_1_0_0": 0.5562,
    "math_dataset_algebra__linear_1d_1_0_0, niv2_sentence_composition": 0.5554,
    "quac_1_0_0, cot_ecqa_ii": 0.5552,
    "fix_punct, airoboros": 0.5541,
    "super_glue_wic_1_0_2, wiki_bio_guess_person": 0.5537,
    "math_dataset_algebra__linear_1d_1_0_0, high_school_government_and_politics": 0.5533,
    "sciq_Direct_Question, wiki_hop_original_choose_best_object_affirmative_1": 0.5533,
    "anatomy, guanaco": 0.5527,
    "human_aging, fix_punct": 0.5519,
    "cot_esnli, professional_medicine": 0.5512,
    "cot_ecqa_ii, ropes_background_situation_middle": 0.5511,
    "fix_punct, niv2_discourse_connective_identification": 0.5511,
    "niv2_code_to_text, niv2_grammar_error_detection": 0.5499,
    "fix_punct, high_school_government_and_politics": 0.5494,
    "super_glue_wic_1_0_2, coqa_1_0_0": 0.5485,
    "human_aging, airoboros": 0.5481,
    "anatomy, cot_ecqa_ii": 0.5475,
    "sciq_Direct_Question, niv2_grammar_error_detection": 0.5475,
    "college_medicine, wiki_hop_original_choose_best_object_affirmative_2": 0.5471,
    "niv2_sentence_composition, wiki_hop_original_choose_best_object_affirmative_2": 0.5466,
    "anatomy, coqa_1_0_0": 0.5465,
    "fix_punct, wiki_bio_guess_person": 0.5465,
    "anatomy, math_dataset_algebra__linear_1d_1_0_0": 0.5462,
    "ropes_prompt_bottom_no_hint, wiki_hop_original_choose_best_object_affirmative_1": 0.5456,
    "cot_strategyqa, wiki_hop_original_choose_best_object_affirmative_2": 0.5456,
    "cot_esnli, human_aging": 0.5444,
    "professional_medicine, web_questions_whats_the_answer": 0.5444,
    "super_glue_wic_1_0_2, web_questions_whats_the_answer": 0.5441,
    "professional_medicine, niv2_discourse_connective_identification": 0.5435,
    "ropes_prompt_bottom_no_hint, wiki_bio_guess_person": 0.5434,
    "college_medicine, cot_ecqa_ii": 0.5434,
    "super_glue_wic_1_0_2, niv2_sentence_composition": 0.5419,
    "ropes_new_situation_background_answer, niv2_sentence_composition": 0.5418,
    "professional_medicine, math_dataset_algebra__linear_1d_1_0_0": 0.5417,
    "professional_medicine, wiki_hop_original_choose_best_object_affirmative_1": 0.5417,
    "web_questions_whats_the_answer, niv2_grammar_error_detection": 0.5403,
    "ropes_new_situation_background_answer, niv2_discourse_connective_identification": 0.5401,
    "niv2_ethics_classification, quac_1_0_0": 0.54,
    "ropes_prompt_bottom_no_hint, airoboros": 0.5396,
    "cot_esnli, niv2_discourse_connective_identification": 0.5384,
    "sciq_Direct_Question, guanaco": 0.5379,
    "college_medicine, math_dataset_algebra__linear_1d_1_0_0": 0.5375,
    "sciq_Direct_Question, sociology": 0.5374,
    "super_glue_wic_1_0_2, cot_strategyqa": 0.5372,
    "quac_1_0_0, niv2_code_to_text": 0.5372,
    "coqa_1_0_0, ropes_background_situation_middle": 0.537,
    "quac_1_0_0, niv2_sentence_composition": 0.5368,
    "college_medicine, sciq_Direct_Question": 0.5364,
    "anatomy, fix_punct": 0.5356,
    "professional_medicine, cot_ecqa_ii": 0.5349,
    "anatomy, sciq_Direct_Question": 0.5347,
    "cot_ecqa_ii, coqa_1_0_0": 0.5338,
    "wiqa_what_might_be_the_first_step_of_the_process, cot_ecqa_ii": 0.5337,
    "niv2_discourse_connective_identification, coqa_1_0_0": 0.5291,
    "high_school_government_and_politics, wiki_hop_original_choose_best_object_affirmative_2": 0.5287,
    "cot_esnli, web_questions_whats_the_answer": 0.5261,
    "super_glue_wic_1_0_2, ropes_background_situation_middle": 0.5261,
    "cot_esnli, guanaco": 0.526,
    "niv2_grammar_error_detection, ropes_background_situation_middle": 0.5257,
    "wiki_hop_original_choose_best_object_affirmative_2, sociology": 0.5243,
    "guanaco, sociology": 0.5239,
    "professional_medicine, fix_punct": 0.5238,
    "anatomy, wiki_hop_original_choose_best_object_affirmative_2": 0.5236,
    "anatomy, ropes_new_situation_background_answer": 0.5229,
    "fix_punct, sociology": 0.5229,
    "professional_medicine, sciq_Direct_Question": 0.5223,
    "wiki_hop_original_choose_best_object_affirmative_2, niv2_discourse_connective_identification": 0.5222,
    "professional_medicine, coqa_1_0_0": 0.5221,
    "ropes_new_situation_background_answer, fix_punct": 0.5221,
    "ropes_prompt_bottom_no_hint, web_questions_whats_the_answer": 0.5216,
    "wiqa_what_might_be_the_first_step_of_the_process, niv2_code_to_text": 0.5215,
    "niv2_ethics_classification, high_school_government_and_politics": 0.5198,
    "high_school_government_and_politics, coqa_1_0_0": 0.5194,
    "professional_medicine, ropes_new_situation_background_answer": 0.519,
    "cot_strategyqa, niv2_code_to_text": 0.519,
    "niv2_ethics_classification, wiki_hop_original_choose_best_object_affirmative_2": 0.5187,
    "web_questions_whats_the_answer, niv2_code_to_text": 0.5187,
    "cot_esnli, quac_1_0_0": 0.5175,
    "cot_esnli, niv2_code_to_text": 0.5172,
    "ropes_new_situation_background_answer, high_school_government_and_politics": 0.5168,
    "niv2_ethics_classification, professional_medicine": 0.5163,
    "niv2_grammar_error_detection, airoboros": 0.516,
    "niv2_sentence_composition, niv2_code_to_text": 0.5154,
    "niv2_code_to_text, ropes_background_situation_middle": 0.515,
    "ropes_prompt_bottom_no_hint, wiki_hop_original_choose_best_object_affirmative_2": 0.5141,
    "niv2_ethics_classification, ropes_new_situation_background_answer": 0.5131,
    "sciq_Direct_Question, wiki_hop_original_choose_best_object_affirmative_2": 0.5129,
    "ropes_prompt_bottom_no_hint, sciq_Direct_Question": 0.5126,
    "ropes_new_situation_background_answer, niv2_code_to_text": 0.5126,
    "niv2_ethics_classification, anatomy": 0.5122,
    "quac_1_0_0, fix_punct": 0.5115,
    "fix_punct, niv2_sentence_composition": 0.5115,
    "web_questions_whats_the_answer, quac_1_0_0": 0.5114,
    "professional_medicine, wiki_hop_original_choose_best_object_affirmative_2": 0.5104,
    "college_medicine, ropes_new_situation_background_answer": 0.5104,
    "anatomy, wiki_bio_guess_person": 0.5093,
    "wiki_bio_guess_person, guanaco": 0.5085,
    "super_glue_wic_1_0_2, wiki_hop_original_choose_best_object_affirmative_1": 0.5076,
    "niv2_ethics_classification, wiki_bio_guess_person": 0.507,
    "niv2_ethics_classification, human_aging": 0.5065,
    "super_glue_wic_1_0_2, wiki_hop_original_choose_best_object_affirmative_2": 0.5064,
    "high_school_government_and_politics, wiki_bio_guess_person": 0.5057,
    "ropes_prompt_bottom_no_hint, math_dataset_algebra__linear_1d_1_0_0": 0.5053,
    "niv2_ethics_classification, college_medicine": 0.5052,
    "college_medicine, fix_punct": 0.5028,
    "cot_ecqa_ii, niv2_code_to_text": 0.5022,
    "math_dataset_algebra__linear_1d_1_0_0, sociology": 0.502,
    "cot_esnli, ropes_prompt_bottom_no_hint": 0.5015,
    "ropes_prompt_bottom_no_hint, niv2_discourse_connective_identification": 0.501,
    "cot_esnli, wiki_hop_original_choose_best_object_affirmative_1": 0.4999,
    "wiqa_what_might_be_the_first_step_of_the_process, fix_punct": 0.4963,
    "professional_medicine, wiki_bio_guess_person": 0.4963,
    "niv2_ethics_classification, sociology": 0.4962,
    "cot_ecqa_ii, niv2_grammar_error_detection": 0.4959,
    "human_aging, coqa_1_0_0": 0.4956,
    "anatomy, niv2_code_to_text": 0.4948,
    "cot_strategyqa, guanaco": 0.4944,
    "super_glue_wic_1_0_2, high_school_government_and_politics": 0.4941,
    "cot_ecqa_ii, wiki_bio_guess_person": 0.4941,
    "high_school_government_and_politics, niv2_code_to_text": 0.4927,
    "ropes_new_situation_background_answer, sociology": 0.4923,
    "cot_strategyqa, wiki_bio_guess_person": 0.4918,
    "professional_medicine, niv2_code_to_text": 0.4913,
    "wiqa_what_might_be_the_first_step_of_the_process, super_glue_wic_1_0_2": 0.4899,
    "ropes_prompt_bottom_no_hint, coqa_1_0_0": 0.4893,
    "niv2_ethics_classification, ropes_prompt_bottom_no_hint": 0.4872,
    "wiqa_what_might_be_the_first_step_of_the_process, quac_1_0_0": 0.4872,
    "ropes_prompt_bottom_no_hint, niv2_code_to_text": 0.4869,
    "super_glue_wic_1_0_2, professional_medicine": 0.4863,
    "college_medicine, niv2_code_to_text": 0.4846,
    "cot_ecqa_ii, wiki_hop_original_choose_best_object_affirmative_1": 0.4824,
    "cot_strategyqa, niv2_grammar_error_detection": 0.4807,
    "cot_esnli, fix_punct": 0.4804,
    "super_glue_wic_1_0_2, sociology": 0.4794,
    "wiki_hop_original_choose_best_object_affirmative_1, guanaco": 0.4773,
    "sciq_Direct_Question, human_aging": 0.4756,
    "college_medicine, wiki_bio_guess_person": 0.4739,
    "anatomy, wiqa_what_might_be_the_first_step_of_the_process": 0.473,
    "wiki_hop_original_choose_best_object_affirmative_1, wiki_bio_guess_person": 0.473,
    "high_school_government_and_politics, niv2_grammar_error_detection": 0.4726,
    "web_questions_whats_the_answer, coqa_1_0_0": 0.4725,
    "human_aging, wiki_hop_original_choose_best_object_affirmative_1": 0.4715,
    "wiqa_what_might_be_the_first_step_of_the_process, sociology": 0.4706,
    "ropes_new_situation_background_answer, guanaco": 0.4701,
    "human_aging, niv2_discourse_connective_identification": 0.4697,
    "wiki_hop_original_choose_best_object_affirmative_2, guanaco": 0.4695,
    "wiqa_what_might_be_the_first_step_of_the_process, ropes_prompt_bottom_no_hint": 0.4677,
    "niv2_code_to_text, sociology": 0.4672,
    "cot_ecqa_ii, guanaco": 0.4669,
    "anatomy, super_glue_wic_1_0_2": 0.4661,
    "cot_esnli, wiki_hop_original_choose_best_object_affirmative_2": 0.465,
    "web_questions_whats_the_answer, wiki_bio_guess_person": 0.4596,
    "math_dataset_algebra__linear_1d_1_0_0, human_aging": 0.4596,
    "professional_medicine, niv2_grammar_error_detection": 0.4573,
    "wiqa_what_might_be_the_first_step_of_the_process, college_medicine": 0.4572,
    "ropes_prompt_bottom_no_hint, cot_ecqa_ii": 0.4568,
    "wiki_hop_original_choose_best_object_affirmative_1, niv2_grammar_error_detection": 0.4564,
    "ropes_new_situation_background_answer, cot_ecqa_ii": 0.4555,
    "wiqa_what_might_be_the_first_step_of_the_process, high_school_government_and_politics": 0.455,
    "wiqa_what_might_be_the_first_step_of_the_process, professional_medicine": 0.4544,
    "wiki_hop_original_choose_best_object_affirmative_1, niv2_code_to_text": 0.4542,
    "wiqa_what_might_be_the_first_step_of_the_process, niv2_discourse_connective_identification": 0.4541,
    "niv2_grammar_error_detection, guanaco": 0.4534,
    "web_questions_whats_the_answer, ropes_new_situation_background_answer": 0.4513,
    "wiki_bio_guess_person, sociology": 0.4504,
    "wiqa_what_might_be_the_first_step_of_the_process, human_aging": 0.4481,
    "wiqa_what_might_be_the_first_step_of_the_process, web_questions_whats_the_answer": 0.4474,
    "anatomy, niv2_grammar_error_detection": 0.4469,
    "niv2_ethics_classification, wiqa_what_might_be_the_first_step_of_the_process": 0.4466,
    "wiqa_what_might_be_the_first_step_of_the_process, wiki_bio_guess_person": 0.4457,
    "human_aging, wiki_bio_guess_person": 0.4447,
    "human_aging, wiki_hop_original_choose_best_object_affirmative_2": 0.4375,
    "wiki_bio_guess_person, wiki_hop_original_choose_best_object_affirmative_2": 0.4363,
    "human_aging, ropes_new_situation_background_answer": 0.4344,
    "human_aging, guanaco": 0.4342,
    "fix_punct, coqa_1_0_0": 0.4329,
    "niv2_code_to_text, wiki_hop_original_choose_best_object_affirmative_2": 0.4327,
    "web_questions_whats_the_answer, wiki_hop_original_choose_best_object_affirmative_1": 0.4319,
    "cot_esnli, ropes_new_situation_background_answer": 0.4307,
    "cot_esnli, ropes_background_situation_middle": 0.4307,
    "human_aging, niv2_code_to_text": 0.4286,
    "college_medicine, niv2_grammar_error_detection": 0.4271,
    "fix_punct, guanaco": 0.4258,
    "wiqa_what_might_be_the_first_step_of_the_process, ropes_background_situation_middle": 0.4249,
    "cot_ecqa_ii, wiki_hop_original_choose_best_object_affirmative_2": 0.4245,
    "web_questions_whats_the_answer, guanaco": 0.4205,
    "wiqa_what_might_be_the_first_step_of_the_process, niv2_grammar_error_detection": 0.4148,
    "niv2_grammar_error_detection, wiki_hop_original_choose_best_object_affirmative_2": 0.4133,
    "wiqa_what_might_be_the_first_step_of_the_process, cot_strategyqa": 0.4101,
    "wiki_bio_guess_person, coqa_1_0_0": 0.4071,
    "ropes_new_situation_background_answer, niv2_grammar_error_detection": 0.4064,
    "web_questions_whats_the_answer, wiki_hop_original_choose_best_object_affirmative_2": 0.4063,
    "cot_esnli, wiki_bio_guess_person": 0.4015,
    "wiqa_what_might_be_the_first_step_of_the_process, niv2_sentence_composition": 0.3977,
    "fix_punct, wiki_hop_original_choose_best_object_affirmative_1": 0.3928,
    "super_glue_wic_1_0_2, ropes_prompt_bottom_no_hint": 0.3916,
    "super_glue_wic_1_0_2, human_aging": 0.3897,
    "guanaco, ropes_background_situation_middle": 0.3881,
    "niv2_grammar_error_detection, sociology": 0.3785,
    "wiqa_what_might_be_the_first_step_of_the_process, cot_esnli": 0.3756,
    "cot_esnli, niv2_grammar_error_detection": 0.3747,
    "human_aging, niv2_grammar_error_detection": 0.3743,
    "ropes_prompt_bottom_no_hint, guanaco": 0.3705,
    "ropes_prompt_bottom_no_hint, niv2_grammar_error_detection": 0.3649,
    "wiqa_what_might_be_the_first_step_of_the_process, airoboros": 0.364,
    "quac_1_0_0, niv2_grammar_error_detection": 0.3604,
    "ultrachat_19, airoboros": 0.3565,
    "wiqa_what_might_be_the_first_step_of_the_process, ropes_new_situation_background_answer": 0.3554,
    "fix_punct, wiki_hop_original_choose_best_object_affirmative_2": 0.3465,
    "wiqa_what_might_be_the_first_step_of_the_process, guanaco": 0.3231,
    "ultrachat_19, guanaco": 0.3139,
    "wiqa_what_might_be_the_first_step_of_the_process, coqa_1_0_0": 0.3035,
    "ultrachat_19, niv2_sentence_composition": 0.2877,
    "niv2_grammar_error_detection, coqa_1_0_0": 0.2845,
    "ultrachat_19, niv2_discourse_connective_identification": 0.2752,
    "wiqa_what_might_be_the_first_step_of_the_process, wiki_hop_original_choose_best_object_affirmative_1": 0.2634,
    "ultrachat_19, coqa_1_0_0": 0.2529,
    "super_glue_wic_1_0_2, ultrachat_19": 0.2434,
    "ultrachat_19, cot_strategyqa": 0.2414,
    "ultrachat_19, math_dataset_algebra__linear_1d_1_0_0": 0.2403,
    "ultrachat_19, wiki_bio_guess_person": 0.2364,
    "ultrachat_19, quac_1_0_0": 0.2353,
    "ultrachat_19, cot_ecqa_ii": 0.2274,
    "ultrachat_19, niv2_grammar_error_detection": 0.2266,
    "wiqa_what_might_be_the_first_step_of_the_process, wiki_hop_original_choose_best_object_affirmative_2": 0.2253,
    "ultrachat_19, sciq_Direct_Question": 0.2219,
    "niv2_ethics_classification, ultrachat_19": 0.2187,
    "ultrachat_19, niv2_code_to_text": 0.2133,
    "professional_medicine, ultrachat_19": 0.2128,
    "web_questions_whats_the_answer, ultrachat_19": 0.212,
    "ultrachat_19, ropes_background_situation_middle": 0.2103,
    "ultrachat_19, human_aging": 0.2102,
    "ultrachat_19, fix_punct": 0.2072,
    "cot_esnli, ultrachat_19": 0.2058,
    "ultrachat_19, high_school_government_and_politics": 0.2057,
    "ropes_prompt_bottom_no_hint, ultrachat_19": 0.1997,
    "ultrachat_19, college_medicine": 0.1985,
    "anatomy, ultrachat_19": 0.1949,
    "ultrachat_19, ropes_new_situation_background_answer": 0.1826,
    "ultrachat_19, wiki_hop_original_choose_best_object_affirmative_1": 0.1806,
    "ultrachat_19, wiki_hop_original_choose_best_object_affirmative_2": 0.1752,
    "ultrachat_19, sociology": 0.1708,
}
# Additional pairs:
# TODO: need to run exp more extensively with 30 completely randomly sampled pairs -> will result in many runs
# pairs:
ANOTHER_30_PAIRS = {
    "ultrachat_19, wiqa_what_is_the_missing_first_step": 0.1696,
    "ultrachat_19, dbpedia_14_pick_one_category_for_the_following_text": 0.1828,
    "ultrachat_19, abstract_algebra": 0.1752,
    "ultrachat_19, cos_e_v1_11_description_question_option_id": 0.1886,
    "ultrachat_19, glue_cola_2_0_0": 0.1957,
    "ultrachat_19, quoref_Found_Context_Online": 0.1927,
    "adversarial_qa_droberta_based_on, race_high_Write_a_multi_choice_question_for_the_following_article": 0.3773,
    "ultrachat_15, public_relations": 0.3227,
    "race_high_Select_the_best_answer_no_instructions_, ultrachat_11": 0.2941,
    "niv2_fact_verification, abstract_algebra": 0.3587,
    "math_dataset_algebra__linear_1d_1_0_0, ultrachat_29": 0.3387,
    "web_questions_whats_the_answer, wiki_bio_who": 0.399,
    "qasc_qa_with_separated_facts_5, dream_generate_first_utterance": 0.5003,
    "race_middle_Select_the_best_answer_generate_span_, cos_e_v1_11_description_question_option_id": 0.4778,
    "cos_e_v1_11_description_question_option_text, glue_mnli_2_0_0": 0.4874,
    "wiki_qa_automatic_system, ropes_plain_bottom_hint": 0.5545,
    "duorc_ParaphraseRC_decide_worth_it, wiki_bio_what_content": 0.4207,
    "niv2_word_analogy, quail_description_context_question_text": 0.4474,
    "sciq_Multiple_Choice, social_i_qa_I_was_wondering": 0.6378,
    "squad_v2_0_3_0_0, human_sexuality": 0.677,
    "niv2_fact_verification, app_reviews_convert_to_rating": 0.6616,
    "niv2_question_rewriting, niv2_sentiment_analysis": 0.6601,
    "niv2_cause_effect_classification, niv2_question_rewriting": 0.76,
    "adversarial_qa_dbidaf_answer_the_following_q, quoref_Given_Context_Answer_Question": 0.7619,
    "conceptual_physics, human_sexuality": 0.9334,
    "professional_psychology, high_school_computer_science": 0.8816,
    "high_school_world_history, high_school_computer_science": 0.819,
    "human_aging, nutrition": 0.9187,
    "cos_e_v1_11_i_think, cos_e_v1_11_explain_why_human": 0.8495,
    "conceptual_physics, college_computer_science": 0.9012,
}
# tasks: 42 new tasks
NEW_TASKS = {
    "abstract_algebra",
    "adversarial_qa_dbidaf_answer_the_following_q",
    "adversarial_qa_droberta_based_on",
    "app_reviews_convert_to_rating",
    "college_computer_science",
    "conceptual_physics",
    "cos_e_v1_11_description_question_option_id",
    "cos_e_v1_11_description_question_option_text",
    "cos_e_v1_11_explain_why_human",
    "cos_e_v1_11_i_think",
    "dbpedia_14_pick_one_category_for_the_following_text",
    "dream_generate_first_utterance",
    "duorc_ParaphraseRC_decide_worth_it",
    "glue_cola_2_0_0",
    "glue_mnli_2_0_0",
    "high_school_computer_science",
    "high_school_world_history",
    "human_sexuality",
    "niv2_cause_effect_classification",
    "niv2_fact_verification",
    "niv2_question_rewriting",
    "niv2_sentiment_analysis",
    "niv2_word_analogy",
    "nutrition",
    "professional_psychology",
    "public_relations",
    "qasc_qa_with_separated_facts_5",
    "quail_description_context_question_text",
    "quoref_Found_Context_Online",
    "quoref_Given_Context_Answer_Question",
    "race_high_Select_the_best_answer_no_instructions_",
    "race_high_Write_a_multi_choice_question_for_the_following_article",
    "race_middle_Select_the_best_answer_generate_span_",
    "ropes_plain_bottom_hint",
    "sciq_Multiple_Choice",
    "social_i_qa_I_was_wondering",
    "squad_v2_0_3_0_0",
    "ultrachat_11",
    "ultrachat_15",
    "ultrachat_29",
    "wiki_bio_what_content",
    "wiki_bio_who",
    "wiki_qa_automatic_system",
    "wiqa_what_is_the_missing_first_step",
}
# 1 train modules for each of these tasks, 4 pochs per module
# train pairwise joint modules for 2 epochs each


# similar sequences: discovered through cos similarity of phi2 (fc[12])
similar10 = [
    "ropes_background_new_situation_answer",
    "wiki_hop_original_generate_object",
    "ropes_new_situation_background_answer",
    "ropes_prompt_beginning",
    "ropes_read_background_situation",
    "ropes_plain_bottom_hint",
    "quarel_heres_a_story",
    "wiki_hop_original_generate_subject",
    "social_i_qa_Generate_the_question_from_the_answer",
    "ropes_background_situation_middle",
]

similar11 = [
    "natural_questions_open_1_0_0",
    "web_questions_whats_the_answer",
    "web_questions_question_answer",
    "kilt_tasks_hotpotqa_combining_facts",
    "web_questions_short_general_knowledge_q",
    "kilt_tasks_hotpotqa_straighforward_qa",
    "web_questions_get_the_answer",
    "kilt_tasks_hotpotqa_complex_question",
    "web_questions_potential_correct_answer",
    "trivia_qa_rc_1_1_0",
    "kilt_tasks_hotpotqa_formulate",
]

# most dissimilar sequences: discovered through cos similarity of phi2 (fc[12])
distinct10 = [
    "wiqa_what_is_the_final_step_of_the_following_process",
    "duorc_SelfRC_generate_question_by_answer",
    "super_glue_cb_1_0_2",
    "sciq_Multiple_Choice",
    "ultrachat_25",
    "niv2_explanation",
    "aeslc_1_0_0",
    "social_i_qa_Check_if_a_random_answer_is_valid_or_not",
    "high_school_psychology",
    "niv2_dialogue_act_recognition",
]

distinct12 = [
    "us_foreign_policy",
    "quartz_answer_question_based_on",
    "niv2_paraphrasing",
    "wiki_qa_Direct_Answer_to_Question",
    "wiki_hop_original_generate_object",
    "wiqa_does_the_supposed_perturbation_have_an_effect",
    "unified_qa_science_inst",
    "quoref_Answer_Test",
    "wiki_hop_original_choose_best_object_affirmative_1",
    "electrical_engineering",
    "wiki_qa_Jeopardy_style",
    "ultrachat_16",
]

# TODO: run this on GPT 1B
# need a data-less merigng baseline
# Then later on Phi
