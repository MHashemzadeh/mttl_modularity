from dataclasses import dataclass
from mttl.config import Config
import os
import json

tasks_names_to_ids = {
    "web_questions_question_answer": 0,
    "cos_e_v1_11_question_option_description_id": 1,
    "squad_v1_1_3_0_0": 2,
    "wmt14_translate_fr_en_1_0_0": 3,
    "cos_e_v1_11_explain_why_human": 4,
    "imdb_reviews_plain_text_1_0_0": 5,
    "wiki_bio_what_content": 6,
    "gem_wiki_lingua_english_en_1_1_0": 7,
    "anli_r1_0_1_0": 8,
    "super_glue_multirc_1_0_2": 9,
    "quartz_answer_question_based_on": 10,
    "adversarial_qa_dbidaf_generate_question": 11,
    "ropes_background_situation_middle": 12,
    "race_high_Select_the_best_answer_no_instructions_": 13,
    "race_high_Write_a_multi_choice_question_options_given_": 14,
    "quail_description_context_question_text": 15,
    "adversarial_qa_droberta_generate_question": 16,
    "ropes_plain_bottom_hint": 17,
    "wiki_hop_original_choose_best_object_interrogative_1": 18,
    "qasc_qa_with_combined_facts_1": 19,
    "dream_read_the_following_conversation_and_answer_the_question": 20,
    "dbpedia_14_given_a_choice_of_categories_": 21,
    "true_case": 22,
    "cosmos_qa_1_0_0": 23,
    "multi_news_1_0_0": 24,
    "app_reviews_categorize_rating_using_review": 25,
    "cos_e_v1_11_rationale": 26,
    "adversarial_qa_dbert_answer_the_following_q": 27,
    "race_middle_Select_the_best_answer_no_instructions_": 28,
    "wiki_qa_automatic_system": 29,
    "cnn_dailymail_3_4_0": 30,
    "quoref_Given_Context_Answer_Question": 31,
    "wiqa_effect_with_string_answer": 32,
    "race_high_Is_this_the_right_answer": 33,
    "quail_context_question_answer_description_text": 34,
    "web_questions_short_general_knowledge_q": 35,
    "cos_e_v1_11_aligned_with_common_sense": 36,
    "qasc_qa_with_separated_facts_3": 37,
    "ai2_arc_ARC_Challenge_1_0_0": 38,
    "gem_web_nlg_en_1_1_0": 39,
    "quarel_heres_a_story": 40,
    "math_dataset_algebra__linear_1d_1_0_0": 41,
    "quarel_logic_test": 42,
    "dream_answer_to_dialogue": 43,
    "ropes_plain_background_situation": 44,
    "anli_r2_0_1_0": 45,
    "super_glue_copa_1_0_2": 46,
    "duorc_ParaphraseRC_answer_question": 47,
    "social_i_qa_I_was_wondering": 48,
    "wiki_qa_Decide_good_answer": 49,
    "qasc_qa_with_separated_facts_4": 50,
    "glue_wnli_2_0_0": 51,
    "wiki_hop_original_choose_best_object_affirmative_1": 52,
    "wiki_qa_Topic_Prediction_Answer_Only": 53,
    "cos_e_v1_11_description_question_option_id": 54,
    "race_middle_Write_a_multi_choice_question_for_the_following_article": 55,
    "duorc_ParaphraseRC_question_answering": 56,
    "wmt16_translate_ro_en_1_0_0": 57,
    "sciq_Multiple_Choice_Closed_Book_": 58,
    "gem_dart_1_1_0": 59,
    "unified_qa_science_inst": 60,
    "duorc_ParaphraseRC_generate_question_by_answer": 61,
    "social_i_qa_Check_if_a_random_answer_is_valid_or_not": 62,
    "glue_stsb_2_0_0": 63,
    "ropes_prompt_mix": 64,
    "race_high_Write_a_multi_choice_question_for_the_following_article": 65,
    "super_glue_wic_1_0_2": 66,
    "super_glue_rte_1_0_2": 67,
    "app_reviews_convert_to_star_rating": 68,
    "dream_generate_first_utterance": 69,
    "quac_1_0_0": 70,
    "ropes_prompt_beginning": 71,
    "quoref_Answer_Friend_Question": 72,
    "wiki_qa_Is_This_True_": 73,
    "duorc_SelfRC_build_story_around_qa": 74,
    "duorc_ParaphraseRC_title_generation": 75,
    "web_questions_get_the_answer": 76,
    "duorc_SelfRC_decide_worth_it": 77,
    "wiki_hop_original_generate_subject_and_object": 78,
    "wiki_hop_original_generate_subject": 79,
    "quartz_having_read_above_passage": 80,
    "coqa_1_0_0": 81,
    "adversarial_qa_dbert_based_on": 82,
    "duorc_ParaphraseRC_movie_director": 83,
    "lambada_1_0_0": 84,
    "qasc_qa_with_separated_facts_1": 85,
    "word_segment": 86,
    "wiki_hop_original_generate_object": 87,
    "app_reviews_generate_review": 88,
    "quoref_Find_Answer": 89,
    "race_high_Select_the_best_answer_generate_span_": 90,
    "duorc_ParaphraseRC_generate_question": 91,
    "dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to": 92,
    "ropes_given_background_situation": 93,
    "quoref_Found_Context_Online": 94,
    "bool_q_1_0_0": 95,
    "social_i_qa_Show_choices_and_generate_index": 96,
    "quail_description_context_question_answer_id": 97,
    "drop_2_0_0": 98,
    "dream_generate_last_utterance": 99,
    "quoref_Answer_Question_Given_Context": 100,
    "race_high_Read_the_article_and_answer_the_question_no_option_": 101,
    "duorc_ParaphraseRC_build_story_around_qa": 102,
    "app_reviews_convert_to_rating": 103,
    "wiki_hop_original_choose_best_object_affirmative_3": 104,
    "gem_common_gen_1_1_0": 105,
    "quail_no_prompt_text": 106,
    "piqa_1_0_0": 107,
    "social_i_qa_Generate_the_question_from_the_answer": 108,
    "trivia_qa_rc_1_1_0": 109,
    "paws_wiki_1_1_0": 110,
    "wiqa_what_might_be_the_first_step_of_the_process": 111,
    "quarel_testing_students": 112,
    "wiki_hop_original_explain_relation": 113,
    "duorc_SelfRC_extract_answer": 114,
    "super_glue_record_1_0_2": 115,
    "wiki_bio_who": 116,
    "kilt_tasks_hotpotqa_formulate": 117,
    "quail_context_description_question_answer_id": 118,
    "sciq_Multiple_Choice_Question_First": 119,
    "para_crawl_enes": 120,
    "adversarial_qa_dbert_generate_question": 121,
    "qasc_qa_with_separated_facts_5": 122,
    "quartz_given_the_fact_answer_the_q": 123,
    "super_glue_cb_1_0_2": 124,
    "wiki_qa_Generate_Question_from_Topic": 125,
    "ropes_plain_no_background": 126,
    "sciq_Direct_Question_Closed_Book_": 127,
    "kilt_tasks_hotpotqa_complex_question": 128,
    "quartz_use_info_from_question_paragraph": 129,
    "glue_qqp_2_0_0": 130,
    "duorc_ParaphraseRC_decide_worth_it": 131,
    "quail_context_description_question_answer_text": 132,
    "huggingface_xsum": 133,
    "kilt_tasks_hotpotqa_final_exam": 134,
    "wiki_bio_guess_person": 135,
    "social_i_qa_Generate_answer": 136,
    "glue_qnli_2_0_0": 137,
    "cos_e_v1_11_i_think": 138,
    "kilt_tasks_hotpotqa_straighforward_qa": 139,
    "quartz_answer_question_below": 140,
    "quartz_read_passage_below_choose": 141,
    "trec_1_0_0": 142,
    "quail_context_question_description_answer_text": 143,
    "wiki_bio_comprehension": 144,
    "cos_e_v1_11_question_description_option_id": 145,
    "adversarial_qa_dbidaf_tell_what_it_is": 146,
    "quail_context_question_description_answer_id": 147,
    "cos_e_v1_11_question_description_option_text": 148,
    "quail_description_context_question_answer_text": 149,
    "natural_questions_open_1_0_0": 150,
    "web_questions_whats_the_answer": 151,
    "wiki_qa_Jeopardy_style": 152,
    "openbookqa_0_1_0": 153,
    "wiqa_what_is_the_final_step_of_the_following_process": 154,
    "wiki_qa_Direct_Answer_to_Question": 155,
    "snli_1_1_0": 156,
    "sciq_Multiple_Choice": 157,
    "cos_e_v1_11_generate_explanation_given_text": 158,
    "aeslc_1_0_0": 159,
    "adversarial_qa_dbert_tell_what_it_is": 160,
    "ai2_arc_ARC_Easy_1_0_0": 161,
    "race_high_Taking_a_test": 162,
    "quail_context_question_answer_description_id": 163,
    "wiqa_what_might_be_the_last_step_of_the_process": 164,
    "wiqa_effect_with_label_answer": 165,
    "adversarial_qa_dbidaf_answer_the_following_q": 166,
    "glue_mrpc_2_0_0": 167,
    "race_middle_Is_this_the_right_answer": 168,
    "wiqa_which_of_the_following_is_the_supposed_perturbation": 169,
    "wiki_qa_Topic_Prediction_Question_and_Answer_Pair": 170,
    "definite_pronoun_resolution_1_1_0": 171,
    "adversarial_qa_droberta_answer_the_following_q": 172,
    "duorc_SelfRC_answer_question": 173,
    "web_questions_potential_correct_answer": 174,
    "duorc_SelfRC_generate_question_by_answer": 175,
    "wiki_qa_exercise": 176,
    "quail_context_description_question_text": 177,
    "ropes_prompt_bottom_no_hint": 178,
    "duorc_SelfRC_title_generation": 179,
    "qasc_is_correct_1": 180,
    "wiki_bio_key_content": 181,
    "dream_baseline": 182,
    "glue_sst2_2_0_0": 183,
    "wiqa_what_is_the_missing_first_step": 184,
    "ag_news_subset_1_0_0": 185,
    "quarel_do_not_use": 186,
    "dbpedia_14_given_list_what_category_does_the_paragraph_belong_to": 187,
    "duorc_SelfRC_generate_question": 188,
    "anli_r3_0_1_0": 189,
    "wiki_hop_original_choose_best_object_affirmative_2": 190,
    "gem_e2e_nlg_1_1_0": 191,
    "adversarial_qa_droberta_based_on": 192,
    "cos_e_v1_11_question_option_description_text": 193,
    "social_i_qa_Show_choices_and_generate_answer": 194,
    "race_middle_Write_a_multi_choice_question_options_given_": 195,
    "quail_no_prompt_id": 196,
    "ropes_prompt_bottom_hint_beginning": 197,
    "squad_v2_0_3_0_0": 198,
    "sciq_Direct_Question": 199,
    "kilt_tasks_hotpotqa_combining_facts": 200,
    "ropes_new_situation_background_answer": 201,
    "quoref_Guess_Answer": 202,
    "gigaword_1_2_0": 203,
    "adversarial_qa_droberta_question_context_answer": 204,
    "wiqa_does_the_supposed_perturbation_have_an_effect": 205,
    "wiki_qa_Topic_Prediction_Question_Only": 206,
    "duorc_SelfRC_movie_director": 207,
    "quail_context_question_description_text": 208,
    "fix_punct": 209,
    "race_middle_Select_the_best_answer": 210,
    "wiki_qa_found_on_google": 211,
    "wmt16_translate_tr_en_1_0_0": 212,
    "wiki_hop_original_choose_best_object_interrogative_2": 213,
    "glue_mnli_2_0_0": 214,
    "quoref_Guess_Title_For_Context": 215,
    "duorc_SelfRC_question_answering": 216,
    "duorc_ParaphraseRC_extract_answer": 217,
    "wmt16_translate_de_en_1_0_0": 218,
    "quoref_Answer_Test": 219,
    "super_glue_wsc_fixed_1_0_2": 220,
    "quartz_paragraph_question_plain_concat": 221,
    "adversarial_qa_dbert_question_context_answer": 222,
    "dbpedia_14_pick_one_category_for_the_following_text": 223,
    "race_middle_Select_the_best_answer_generate_span_": 224,
    "quoref_What_Is_The_Answer": 225,
    "cos_e_v1_11_description_question_option_text": 226,
    "quoref_Read_And_Extract_": 227,
    "wmt16_translate_fi_en_1_0_0": 228,
    "adversarial_qa_dbidaf_question_context_answer": 229,
    "hellaswag_1_1_0": 230,
    "qasc_qa_with_separated_facts_2": 231,
    "winogrande_1_1_0": 232,
    "glue_cola_2_0_0": 233,
    "race_high_Select_the_best_answer": 234,
    "ropes_read_background_situation": 235,
    "race_middle_Read_the_article_and_answer_the_question_no_option_": 236,
    "quoref_Context_Contains_Answer": 237,
    "quarel_choose_between": 238,
    "ropes_background_new_situation_answer": 239,
    "quartz_use_info_from_paragraph_question": 240,
    "adversarial_qa_dbidaf_based_on": 241,
    "race_middle_Taking_a_test": 242,
    "yelp_polarity_reviews_0_2_0": 243,
    "adversarial_qa_droberta_tell_what_it_is": 244,
    "qasc_is_correct_2": 245,
}

tasks_names_to_ids_ada = {
    "quarel_testing_students": 0,
    "quoref_Guess_Title_For_Context": 1,
    "ultrachat_21": 2,
    "adversarial_qa_dbidaf_question_context_answer": 3,
    "cos_e_v1_11_question_option_description_id": 4,
    "wiki_qa_Jeopardy_style": 5,
    "super_glue_rte_1_0_2": 6,
    "cot_strategyqa_ii": 7,
    "wiki_bio_key_content": 8,
    "ultrachat_3": 9,
    "reclor": 10,
    "cos_e_v1_11_generate_explanation_given_text": 11,
    "niv2_translation": 12,
    "niv2_poem_generation": 13,
    "adversarial_qa_dbert_question_context_answer": 14,
    "adversarial_qa_dbert_based_on": 15,
    "quoref_Given_Context_Answer_Question": 16,
    "wiqa_what_might_be_the_last_step_of_the_process": 17,
    "app_reviews_convert_to_rating": 18,
    "web_questions_get_the_answer": 19,
    "niv2_dialogue_act_recognition": 20,
    "ropes_background_new_situation_answer": 21,
    "qasc_qa_with_separated_facts_1": 22,
    "wiqa_does_the_supposed_perturbation_have_an_effect": 23,
    "quoref_Find_Answer": 24,
    "miscellaneous": 25,
    "niv2_paper_review": 26,
    "high_school_world_history": 27,
    "super_glue_multirc_1_0_2": 28,
    "ultrachat_22": 29,
    "gem_dart_1_1_0": 30,
    "sciq_Multiple_Choice_Closed_Book_": 31,
    "duorc_SelfRC_generate_question": 32,
    "hellaswag_1_1_0": 33,
    "qasc_is_correct_2": 34,
    "niv2_text_simplification": 35,
    "tigerbot-kaggle": 36,
    "adversarial_qa_droberta_answer_the_following_q": 37,
    "glue_sst2_2_0_0": 38,
    "niv2_explanation": 39,
    "duorc_SelfRC_generate_question_by_answer": 40,
    "wiki_qa_found_on_google": 41,
    "race_high_Read_the_article_and_answer_the_question_no_option_": 42,
    "philosophy": 43,
    "high_school_us_history": 44,
    "fix_punct": 45,
    "moral_scenarios": 46,
    "ultrachat_30": 47,
    "cos_e_v1_11_aligned_with_common_sense": 48,
    "duorc_SelfRC_movie_director": 49,
    "ultrachat_11": 50,
    "wiki_bio_comprehension": 51,
    "app_reviews_categorize_rating_using_review": 52,
    "drop_2_0_0": 53,
    "squad_v2_0_3_0_0": 54,
    "global_facts": 55,
    "niv2_named_entity_recognition": 56,
    "sciq_Multiple_Choice": 57,
    "quail_context_question_answer_description_text": 58,
    "wmt16_translate_fi_en_1_0_0": 59,
    "niv2_grammar_error_correction": 60,
    "quoref_Context_Contains_Answer": 61,
    "niv2_title_generation": 62,
    "huggingface_xsum": 63,
    "virology": 64,
    "duorc_ParaphraseRC_question_answering": 65,
    "ropes_prompt_beginning": 66,
    "dream_generate_first_utterance": 67,
    "niv2_fact_verification": 68,
    "ultrachat_4": 69,
    "niv2_mathematics": 70,
    "social_i_qa_Generate_the_question_from_the_answer": 71,
    "para_crawl_enes": 72,
    "cos_e_v1_11_description_question_option_text": 73,
    "race_middle_Read_the_article_and_answer_the_question_no_option_": 74,
    "paws_wiki_1_1_0": 75,
    "formal_logic": 76,
    "niv2_sentence_expansion": 77,
    "niv2_summarization": 78,
    "bool_q_1_0_0": 79,
    "quartz_given_the_fact_answer_the_q": 80,
    "stream_aqua_ii": 81,
    "anli_r2_0_1_0": 82,
    "high_school_biology": 83,
    "ropes_prompt_bottom_no_hint": 84,
    "social_i_qa_Check_if_a_random_answer_is_valid_or_not": 85,
    "ropes_plain_bottom_hint": 86,
    "race_high_Select_the_best_answer": 87,
    "management": 88,
    "clinical_knowledge": 89,
    "race_high_Write_a_multi_choice_question_for_the_following_article": 90,
    "duorc_SelfRC_extract_answer": 91,
    "ultrachat_0": 92,
    "adversarial_qa_dbert_generate_question": 93,
    "duorc_ParaphraseRC_title_generation": 94,
    "niv2_gender_classification": 95,
    "social_i_qa_Show_choices_and_generate_index": 96,
    "niv2_text_completion": 97,
    "ultrachat_20": 98,
    "scibench": 99,
    "cot_creak": 100,
    "dbpedia_14_given_list_what_category_does_the_paragraph_belong_to": 101,
    "imdb_reviews_plain_text_1_0_0": 102,
    "computer_security": 103,
    "quoref_Guess_Answer": 104,
    "niv2_entity_relation_classification": 105,
    "unified_qa_science_inst": 106,
    "human_sexuality": 107,
    "adversarial_qa_dbert_answer_the_following_q": 108,
    "dream_answer_to_dialogue": 109,
    "cot_ecqa": 110,
    "niv2_answerability_classification": 111,
    "quartz_use_info_from_paragraph_question": 112,
    "math_dataset_algebra__linear_1d_1_0_0": 113,
    "glue_mrpc_2_0_0": 114,
    "professional_psychology": 115,
    "gem_common_gen_1_1_0": 116,
    "quarel_logic_test": 117,
    "niv2_word_semantics": 118,
    "niv2_question_rewriting": 119,
    "niv2_section_classification": 120,
    "wiki_hop_original_choose_best_object_interrogative_2": 121,
    "wiki_hop_original_choose_best_object_affirmative_1": 122,
    "quartz_answer_question_based_on": 123,
    "medical_genetics": 124,
    "race_middle_Select_the_best_answer_no_instructions_": 125,
    "web_questions_potential_correct_answer": 126,
    "niv2_toxic_language_detection": 127,
    "niv2_story_composition": 128,
    "cot_gsm8k_ii": 129,
    "glue_mnli_2_0_0": 130,
    "niv2_preposition_prediction": 131,
    "ultrachat_1": 132,
    "niv2_word_relation_classification": 133,
    "high_school_european_history": 134,
    "ultrachat_16": 135,
    "wiki_qa_Generate_Question_from_Topic": 136,
    "niv2_language_identification": 137,
    "college_computer_science": 138,
    "ropes_prompt_mix": 139,
    "wiki_hop_original_generate_subject_and_object": 140,
    "gigaword_1_2_0": 141,
    "duorc_SelfRC_title_generation": 142,
    "app_reviews_convert_to_star_rating": 143,
    "ultrachat_8": 144,
    "niv2_text_categorization": 145,
    "squad_v1_1_3_0_0": 146,
    "quail_no_prompt_text": 147,
    "niv2_coreference_resolution": 148,
    "ropes_new_situation_background_answer": 149,
    "piqa_1_0_0": 150,
    "super_glue_record_1_0_2": 151,
    "wiki_qa_exercise": 152,
    "trivia_qa_rc_1_1_0": 153,
    "astronomy": 154,
    "moral_disputes": 155,
    "niv2_question_understanding": 156,
    "ultrachat_12": 157,
    "niv2_question_answering": 158,
    "qasc_qa_with_separated_facts_2": 159,
    "sciq_Direct_Question": 160,
    "race_high_Select_the_best_answer_no_instructions_": 161,
    "ropes_given_background_situation": 162,
    "quail_description_context_question_answer_text": 163,
    "guanaco": 164,
    "quoref_Answer_Test": 165,
    "cot_strategyqa": 166,
    "niv2_question_decomposition": 167,
    "adversarial_qa_dbidaf_tell_what_it_is": 168,
    "cot_gsm8k": 169,
    "dream_read_the_following_conversation_and_answer_the_question": 170,
    "niv2_style_transfer": 171,
    "natural_questions_open_1_0_0": 172,
    "social_i_qa_Generate_answer": 173,
    "human_aging": 174,
    "duorc_ParaphraseRC_generate_question_by_answer": 175,
    "true_case": 176,
    "wiki_bio_guess_person": 177,
    "sociology": 178,
    "social_i_qa_Show_choices_and_generate_answer": 179,
    "quail_description_context_question_answer_id": 180,
    "cot_sensemaking_ii": 181,
    "niv2_text_to_code": 182,
    "cot_qasc": 183,
    "world_religions": 184,
    "college_chemistry": 185,
    "niv2_sentiment_analysis": 186,
    "leetcode_ne": 187,
    "niv2_sentence_composition": 188,
    "duorc_ParaphraseRC_movie_director": 189,
    "wiki_qa_Is_This_True_": 190,
    "super_glue_wic_1_0_2": 191,
    "wmt16_translate_tr_en_1_0_0": 192,
    "wiqa_effect_with_string_answer": 193,
    "quartz_paragraph_question_plain_concat": 194,
    "business_ethics": 195,
    "niv2_keyword_tagging": 196,
    "cos_e_v1_11_description_question_option_id": 197,
    "electrical_engineering": 198,
    "niv2_commonsense_classification": 199,
    "quail_context_question_answer_description_id": 200,
    "wiki_qa_Decide_good_answer": 201,
    "dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to": 202,
    "ultrachat_9": 203,
    "wmt16_translate_de_en_1_0_0": 204,
    "quoref_Found_Context_Online": 205,
    "niv2_discourse_connective_identification": 206,
    "niv2_misc": 207,
    "niv2_ethics_classification": 208,
    "niv2_text_matching": 209,
    "theoremqa": 210,
    "quoref_Answer_Question_Given_Context": 211,
    "anatomy": 212,
    "cot_sensemaking": 213,
    "dream_baseline": 214,
    "qasc_is_correct_1": 215,
    "college_physics": 216,
    "niv2_question_generation": 217,
    "niv2_dialogue_generation": 218,
    "niv2_program_execution": 219,
    "niv2_sentence_perturbation": 220,
    "kilt_tasks_hotpotqa_straighforward_qa": 221,
    "qasc_qa_with_separated_facts_4": 222,
    "security_studies": 223,
    "cot_esnli_ii": 224,
    "quarel_heres_a_story": 225,
    "ag_news_subset_1_0_0": 226,
    "niv2_code_to_text": 227,
    "adversarial_qa_dbert_tell_what_it_is": 228,
    "gem_web_nlg_en_1_1_0": 229,
    "niv2_textual_entailment": 230,
    "niv2_spam_classification": 231,
    "race_high_Select_the_best_answer_generate_span_": 232,
    "openbookqa_0_1_0": 233,
    "anli_r3_0_1_0": 234,
    "niv2_data_to_text": 235,
    "web_questions_whats_the_answer": 236,
    "quartz_use_info_from_question_paragraph": 237,
    "race_middle_Is_this_the_right_answer": 238,
    "niv2_fill_in_the_blank": 239,
    "cot_qasc_ii": 240,
    "wiqa_which_of_the_following_is_the_supposed_perturbation": 241,
    "logical_fallacies": 242,
    "niv2_intent_identification": 243,
    "niv2_coherence_classification": 244,
    "snli_1_1_0": 245,
    "duorc_ParaphraseRC_decide_worth_it": 246,
    "winogrande_1_1_0": 247,
    "stream_qed_ii": 248,
    "ultrachat_15": 249,
    "elementary_mathematics": 250,
    "niv2_speaker_relation_classification": 251,
    "multi_news_1_0_0": 252,
    "stream_aqua": 253,
    "super_glue_copa_1_0_2": 254,
    "quarel_choose_between": 255,
    "wiqa_what_is_the_final_step_of_the_following_process": 256,
    "niv2_word_analogy": 257,
    "wiki_qa_Topic_Prediction_Question_Only": 258,
    "gem_e2e_nlg_1_1_0": 259,
    "adversarial_qa_droberta_tell_what_it_is": 260,
    "high_school_statistics": 261,
    "high_school_geography": 262,
    "adversarial_qa_droberta_generate_question": 263,
    "niv2_linguistic_probing": 264,
    "ARB": 265,
    "sciq_Direct_Question_Closed_Book_": 266,
    "wiki_hop_original_choose_best_object_affirmative_2": 267,
    "niv2_cause_effect_classification": 268,
    "professional_law": 269,
    "high_school_psychology": 270,
    "jurisprudence": 271,
    "glue_wnli_2_0_0": 272,
    "ultrachat_23": 273,
    "trec_1_0_0": 274,
    "ai2_arc_ARC_Challenge_1_0_0": 275,
    "ropes_prompt_bottom_hint_beginning": 276,
    "cot_ecqa_ii": 277,
    "race_middle_Write_a_multi_choice_question_for_the_following_article": 278,
    "ai2_arc_ARC_Easy_1_0_0": 279,
    "web_questions_question_answer": 280,
    "niv2_pos_tagging": 281,
    "adversarial_qa_droberta_question_context_answer": 282,
    "prehistory": 283,
    "ultrachat_10": 284,
    "duorc_ParaphraseRC_build_story_around_qa": 285,
    "cot_creak_ii": 286,
    "race_middle_Select_the_best_answer": 287,
    "qasc_qa_with_separated_facts_5": 288,
    "niv2_discourse_relation_classification": 289,
    "quartz_answer_question_below": 290,
    "ultrachat_7": 291,
    "quartz_read_passage_below_choose": 292,
    "duorc_SelfRC_question_answering": 293,
    "niv2_text_quality_evaluation": 294,
    "duorc_ParaphraseRC_generate_question": 295,
    "ropes_plain_no_background": 296,
    "race_high_Taking_a_test": 297,
    "wiki_qa_Topic_Prediction_Answer_Only": 298,
    "stream_qed": 299,
    "cos_e_v1_11_question_description_option_text": 300,
    "wiki_qa_Topic_Prediction_Question_and_Answer_Pair": 301,
    "super_glue_cb_1_0_2": 302,
    "wmt16_translate_ro_en_1_0_0": 303,
    "wiqa_what_is_the_missing_first_step": 304,
    "quail_context_question_description_answer_text": 305,
    "super_glue_wsc_fixed_1_0_2": 306,
    "quail_context_description_question_answer_id": 307,
    "ultrachat_26": 308,
    "high_school_microeconomics": 309,
    "qasc_qa_with_separated_facts_3": 310,
    "web_questions_short_general_knowledge_q": 311,
    "dbpedia_14_pick_one_category_for_the_following_text": 312,
    "glue_stsb_2_0_0": 313,
    "duorc_ParaphraseRC_extract_answer": 314,
    "quail_description_context_question_text": 315,
    "airoboros": 316,
    "app_reviews_generate_review": 317,
    "duorc_SelfRC_answer_question": 318,
    "niv2_paraphrasing": 319,
    "niv2_grammar_error_detection": 320,
    "ropes_background_situation_middle": 321,
    "kilt_tasks_hotpotqa_combining_facts": 322,
    "ultrachat_28": 323,
    "ultrachat_24": 324,
    "race_middle_Select_the_best_answer_generate_span_": 325,
    "quac_1_0_0": 326,
    "nutrition": 327,
    "marketing": 328,
    "quarel_do_not_use": 329,
    "adversarial_qa_droberta_based_on": 330,
    "ultrachat_2": 331,
    "us_foreign_policy": 332,
    "adversarial_qa_dbidaf_answer_the_following_q": 333,
    "ropes_read_background_situation": 334,
    "niv2_irony_detection": 335,
    "professional_accounting": 336,
    "gem_wiki_lingua_english_en_1_1_0": 337,
    "duorc_ParaphraseRC_answer_question": 338,
    "race_middle_Write_a_multi_choice_question_options_given_": 339,
    "race_high_Is_this_the_right_answer": 340,
    "quail_context_question_description_answer_id": 341,
    "abstract_algebra": 342,
    "wiki_hop_original_choose_best_object_interrogative_1": 343,
    "wiki_hop_original_choose_best_object_affirmative_3": 344,
    "quoref_What_Is_The_Answer": 345,
    "public_relations": 346,
    "niv2_number_conversion": 347,
    "niv2_overlap_extraction": 348,
    "econometrics": 349,
    "high_school_mathematics": 350,
    "MATH/PRM-800K": 351,
    "wiki_qa_Direct_Answer_to_Question": 352,
    "niv2_punctuation_error_detection": 353,
    "quoref_Answer_Friend_Question": 354,
    "ultrachat_17": 355,
    "kilt_tasks_hotpotqa_formulate": 356,
    "college_biology": 357,
    "scienceqa": 358,
    "ropes_plain_background_situation": 359,
    "niv2_speaker_identification": 360,
    "cos_e_v1_11_question_option_description_text": 361,
    "cnn_dailymail_3_4_0": 362,
    "niv2_stance_detection": 363,
    "high_school_chemistry": 364,
    "niv2_dialogue_state_tracking": 365,
    "wiki_hop_original_generate_subject": 366,
    "race_middle_Taking_a_test": 367,
    "ultrachat_18": 368,
    "high_school_computer_science": 369,
    "race_high_Write_a_multi_choice_question_options_given_": 370,
    "wmt14_translate_fr_en_1_0_0": 371,
    "anli_r1_0_1_0": 372,
    "ultrachat_31": 373,
    "ultrachat_29": 374,
    "professional_medicine": 375,
    "niv2_answer_verification": 376,
    "kilt_tasks_hotpotqa_complex_question": 377,
    "quoref_Read_And_Extract_": 378,
    "ultrachat_19": 379,
    "cos_e_v1_11_rationale": 380,
    "wiqa_what_might_be_the_first_step_of_the_process": 381,
    "quail_context_description_question_answer_text": 382,
    "wiki_bio_what_content": 383,
    "ultrachat_27": 384,
    "qasc_qa_with_combined_facts_1": 385,
    "niv2_wrong_candidate_generation": 386,
    "cot_esnli": 387,
    "high_school_macroeconomics": 388,
    "cosmos_qa_1_0_0": 389,
    "kilt_tasks_hotpotqa_final_exam": 390,
    "adversarial_qa_dbidaf_based_on": 391,
    "duorc_SelfRC_decide_worth_it": 392,
    "yelp_polarity_reviews_0_2_0": 393,
    "niv2_entity_generation": 394,
    "glue_qqp_2_0_0": 395,
    "dream_generate_last_utterance": 396,
    "quail_no_prompt_id": 397,
    "word_segment": 398,
    "adversarial_qa_dbidaf_generate_question": 399,
    "quail_context_question_description_text": 400,
    "machine_learning": 401,
    "aeslc_1_0_0": 402,
    "duorc_SelfRC_build_story_around_qa": 403,
    "cos_e_v1_11_explain_why_human": 404,
    "college_medicine": 405,
    "wiqa_effect_with_label_answer": 406,
    "sciq_Multiple_Choice_Question_First": 407,
    "cos_e_v1_11_question_description_option_id": 408,
    "wiki_qa_automatic_system": 409,
    "ultrachat_14": 410,
    "glue_cola_2_0_0": 411,
    "cos_e_v1_11_i_think": 412,
    "international_law": 413,
    "high_school_government_and_politics": 414,
    "wiki_hop_original_generate_object": 415,
    "niv2_sentence_ordering": 416,
    "niv2_negotiation_strategy_detection": 417,
    "ultrachat_5": 418,
    "niv2_sentence_compression": 419,
    "definite_pronoun_resolution_1_1_0": 420,
    "wiki_bio_who": 421,
    "high_school_physics": 422,
    "coqa_1_0_0": 423,
    "niv2_stereotype_detection": 424,
    "ultrachat_25": 425,
    "ultrachat_13": 426,
    "college_mathematics": 427,
    "quail_context_description_question_text": 428,
    "dbpedia_14_given_a_choice_of_categories_": 429,
    "wiki_hop_original_explain_relation": 430,
    "social_i_qa_I_was_wondering": 431,
    "niv2_spelling_error_detection": 432,
    "quartz_having_read_above_passage": 433,
    "conceptual_physics": 434,
    "lambada_1_0_0": 435,
    "glue_qnli_2_0_0": 436,
    "niv2_information_extraction": 437,
    "ultrachat_6": 438,
    "default": 439,
}
# convert tasks_names_to_ids to tasks_ids_to_tasks_names
ids_to_tasks_names = {v: k for k, v in tasks_names_to_ids.items()}

ids_to_tasks_names_ada = {v: k for k, v in tasks_names_to_ids_ada.items()}


class ExpertConfig(Config):
    def _set_defaults(self):
        super()._set_defaults()

        self.load_in_8bit = False
        self.wandb_project = None
        self.tensorboard = False
        self.hf_token_hub = None
        self.hf_repo_id = None

        self.expert_name = None
        self.routing = "subject"
        self.mmlu_test_split = "test"
        self.load_module = None
        self.module_graph = None
        self.micro_batch_size = None
        self.validation_portion = 0.03

        self.source_template = None
        self.augment_few_shot = 0

        self.expand_val_set_w_downstream = False

        self.eval_mmlu_callbacks_every = 0
        self.eval_test_set_callback_every = 0
        self.eval_rougeL_callback_every = 0
        self.test_sets_callbacks = []

        self.use_custom_valid_callback = False  # if True use custom callback to early top on eval loss  instead of lightning callback

        self.data_dir = os.getenv("AMLT_DATA_DIR", "~/data/")
        self.output_dir = os.getenv("AMLT_OUTPUT_DIR", "tmp/instruction_learning/")

        # training expert
        self.eval_mmlu_flag = False

        # training classfier routing
        self.num_labels = 246
        self.expert_model_path = None
        self.retrieval_model = "classifier"
        self.expert_library_path = None
        self.text_encoder_trained = False

        self.eval_metric = "loss"
        self.use_vllm = False

    def post_init(self):
        if self.micro_batch_size is None:
            self.micro_batch_size = self.train_batch_size

        # to reproduce setup in https://github.com/daanelson/alpaca-lora
        self.gradient_accumulation_steps = (
            self.train_batch_size // self.micro_batch_size
        )
        self.train_batch_size = self.micro_batch_size
