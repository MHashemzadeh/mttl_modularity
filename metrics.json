{
  "exact_match_default_track": 30.8482,
  "rouge1_default_track": 50.3968,
  "rougeL_default_track": 48.6476,
  "exact_match_for_title_generation_default_track": 12.1813,
  "rouge1_for_title_generation_default_track": 39.1366,
  "rougeL_for_title_generation_default_track": 35.8599,
  "exact_match_for_coreference_resolution_default_track": 31.8571,
  "rouge1_for_coreference_resolution_default_track": 40.7369,
  "rougeL_for_coreference_resolution_default_track": 40.67,
  "exact_match_for_textual_entailment_default_track": 45.9583,
  "rouge1_for_textual_entailment_default_track": 48.5972,
  "rougeL_for_textual_entailment_default_track": 48.5972,
  "exact_match_for_question_rewriting_default_track": 5.1818,
  "rouge1_for_question_rewriting_default_track": 71.6997,
  "rougeL_for_question_rewriting_default_track": 67.184,
  "exact_match_for_cause_effect_classification_default_track": 37.4286,
  "rouge1_for_cause_effect_classification_default_track": 59.6419,
  "rougeL_for_cause_effect_classification_default_track": 58.3106,
  "exact_match_for_dialogue_act_recognition_default_track": 40.7143,
  "rouge1_for_dialogue_act_recognition_default_track": 46.3571,
  "rougeL_for_dialogue_act_recognition_default_track": 46.3571,
  "exact_match_for_answerability_classification_default_track": 58.4615,
  "rouge1_for_answerability_classification_default_track": 61.1282,
  "rougeL_for_answerability_classification_default_track": 61.1282,
  "exact_match_for_keyword_tagging_default_track": 26.2,
  "rouge1_for_keyword_tagging_default_track": 47.2131,
  "rougeL_for_keyword_tagging_default_track": 46.6194,
  "exact_match_for_data_to_text_default_track": 0.9697,
  "rouge1_for_data_to_text_default_track": 54.7365,
  "rougeL_for_data_to_text_default_track": 44.5893,
  "exact_match_for_word_analogy_default_track": 39.5,
  "rouge1_for_word_analogy_default_track": 43.7583,
  "rougeL_for_word_analogy_default_track": 43.7583,
  "exact_match_for_overlap_extraction_default_track": 17.5,
  "rouge1_for_overlap_extraction_default_track": 31.1359,
  "rougeL_for_overlap_extraction_default_track": 30.815,
  "exact_match_for_grammar_error_correction_default_track": 19.0,
  "rouge1_for_grammar_error_correction_default_track": 89.0384,
  "rougeL_for_grammar_error_correction_default_track": 87.889,
  "exact_match_for_task1356_xlsum_title_generation_default_track": 0.0,
  "rouge1_for_task1356_xlsum_title_generation_default_track": 26.9219,
  "rougeL_for_task1356_xlsum_title_generation_default_track": 22.7528,
  "exact_match_for_task893_gap_fill_the_blank_coreference_resolution_default_track": 45.0,
  "rouge1_for_task893_gap_fill_the_blank_coreference_resolution_default_track": 45.0,
  "rougeL_for_task893_gap_fill_the_blank_coreference_resolution_default_track": 45.0,
  "exact_match_for_task641_esnli_classification_default_track": 33.0,
  "rouge1_for_task641_esnli_classification_default_track": 33.0,
  "rougeL_for_task641_esnli_classification_default_track": 33.0,
  "exact_match_for_task1529_scitail1.1_classification_default_track": 68.0,
  "rouge1_for_task1529_scitail1.1_classification_default_track": 68.0,
  "rougeL_for_task1529_scitail1.1_classification_default_track": 68.0,
  "exact_match_for_task202_mnli_contradiction_classification_default_track": 31.0,
  "rouge1_for_task202_mnli_contradiction_classification_default_track": 31.0,
  "rougeL_for_task202_mnli_contradiction_classification_default_track": 31.0,
  "exact_match_for_task670_ambigqa_question_generation_default_track": 2.0,
  "rouge1_for_task670_ambigqa_question_generation_default_track": 74.7523,
  "rougeL_for_task670_ambigqa_question_generation_default_track": 73.1399,
  "exact_match_for_task1393_superglue_copa_text_completion_default_track": 50.0,
  "rouge1_for_task1393_superglue_copa_text_completion_default_track": 50.0,
  "rougeL_for_task1393_superglue_copa_text_completion_default_track": 50.0,
  "exact_match_for_task1344_glue_entailment_classification_default_track": 79.0,
  "rouge1_for_task1344_glue_entailment_classification_default_track": 79.0,
  "rougeL_for_task1344_glue_entailment_classification_default_track": 79.0,
  "exact_match_for_task288_gigaword_summarization_default_track": 0.0,
  "rouge1_for_task288_gigaword_summarization_default_track": 38.484,
  "rougeL_for_task288_gigaword_summarization_default_track": 35.3237,
  "exact_match_for_task1387_anli_r3_entailment_default_track": 33.0,
  "rouge1_for_task1387_anli_r3_entailment_default_track": 33.0,
  "rougeL_for_task1387_anli_r3_entailment_default_track": 33.0,
  "exact_match_for_task1664_winobias_text_generation_default_track": 4.0,
  "rouge1_for_task1664_winobias_text_generation_default_track": 57.7531,
  "rougeL_for_task1664_winobias_text_generation_default_track": 56.8174,
  "exact_match_for_task1161_coda19_title_generation_default_track": 1.0,
  "rouge1_for_task1161_coda19_title_generation_default_track": 40.7643,
  "rougeL_for_task1161_coda19_title_generation_default_track": 32.9524,
  "exact_match_for_task880_schema_guided_dstc8_classification_default_track": 23.0,
  "rouge1_for_task880_schema_guided_dstc8_classification_default_track": 34.3333,
  "rougeL_for_task880_schema_guided_dstc8_classification_default_track": 34.3333,
  "exact_match_for_task738_perspectrum_classification_default_track": 56.0,
  "rouge1_for_task738_perspectrum_classification_default_track": 84.0,
  "rougeL_for_task738_perspectrum_classification_default_track": 84.0,
  "exact_match_for_task1439_doqa_cooking_isanswerable_default_track": 44.0,
  "rouge1_for_task1439_doqa_cooking_isanswerable_default_track": 44.0,
  "rougeL_for_task1439_doqa_cooking_isanswerable_default_track": 44.0,
  "exact_match_for_task645_summarization_default_track": 42.0,
  "rouge1_for_task645_summarization_default_track": 62.8667,
  "rougeL_for_task645_summarization_default_track": 62.8667,
  "exact_match_for_task619_ohsumed_abstract_title_generation_default_track": 2.0,
  "rouge1_for_task619_ohsumed_abstract_title_generation_default_track": 50.3288,
  "rougeL_for_task619_ohsumed_abstract_title_generation_default_track": 42.9919,
  "exact_match_for_task1728_web_nlg_data_to_text_default_track": 6.0,
  "rouge1_for_task1728_web_nlg_data_to_text_default_track": 67.3306,
  "rougeL_for_task1728_web_nlg_data_to_text_default_track": 55.4849,
  "exact_match_for_task1640_aqa1.0_answerable_unanswerable_question_classification_default_track": 64.0,
  "rouge1_for_task1640_aqa1.0_answerable_unanswerable_question_classification_default_track": 64.0,
  "rougeL_for_task1640_aqa1.0_answerable_unanswerable_question_classification_default_track": 64.0,
  "exact_match_for_task648_answer_generation_default_track": 11.0,
  "rouge1_for_task648_answer_generation_default_track": 31.9667,
  "rougeL_for_task648_answer_generation_default_track": 31.9667,
  "exact_match_for_task242_tweetqa_classification_default_track": 86.0,
  "rouge1_for_task242_tweetqa_classification_default_track": 86.0,
  "rougeL_for_task242_tweetqa_classification_default_track": 86.0,
  "exact_match_for_task620_ohsumed_medical_subject_headings_answer_generation_default_track": 21.0,
  "rouge1_for_task620_ohsumed_medical_subject_headings_answer_generation_default_track": 41.1169,
  "rougeL_for_task620_ohsumed_medical_subject_headings_answer_generation_default_track": 40.1966,
  "exact_match_for_task1159_bard_analogical_reasoning_containers_default_track": 46.0,
  "rouge1_for_task1159_bard_analogical_reasoning_containers_default_track": 54.0,
  "rougeL_for_task1159_bard_analogical_reasoning_containers_default_track": 54.0,
  "exact_match_for_task500_scruples_anecdotes_title_generation_default_track": 0.0,
  "rouge1_for_task500_scruples_anecdotes_title_generation_default_track": 22.0735,
  "rougeL_for_task500_scruples_anecdotes_title_generation_default_track": 19.1572,
  "exact_match_for_task890_gcwd_classification_default_track": 1.0,
  "rouge1_for_task890_gcwd_classification_default_track": 1.0,
  "rougeL_for_task890_gcwd_classification_default_track": 1.0,
  "exact_match_for_task039_qasc_find_overlapping_words_default_track": 35.0,
  "rouge1_for_task039_qasc_find_overlapping_words_default_track": 37.6667,
  "rougeL_for_task039_qasc_find_overlapping_words_default_track": 37.6667,
  "exact_match_for_task1154_bard_analogical_reasoning_travel_default_track": 3.0,
  "rouge1_for_task1154_bard_analogical_reasoning_travel_default_track": 15.6667,
  "rougeL_for_task1154_bard_analogical_reasoning_travel_default_track": 15.6667,
  "exact_match_for_task1612_sick_label_classification_default_track": 40.0,
  "rouge1_for_task1612_sick_label_classification_default_track": 40.0,
  "rougeL_for_task1612_sick_label_classification_default_track": 40.0,
  "exact_match_for_task1442_doqa_movies_isanswerable_default_track": 50.0,
  "rouge1_for_task1442_doqa_movies_isanswerable_default_track": 50.0,
  "rougeL_for_task1442_doqa_movies_isanswerable_default_track": 50.0,
  "exact_match_for_task233_iirc_link_exists_classification_default_track": 50.0,
  "rouge1_for_task233_iirc_link_exists_classification_default_track": 50.0,
  "rougeL_for_task233_iirc_link_exists_classification_default_track": 50.0,
  "exact_match_for_task936_defeasible_nli_snli_classification_default_track": 59.0,
  "rouge1_for_task936_defeasible_nli_snli_classification_default_track": 59.0,
  "rougeL_for_task936_defeasible_nli_snli_classification_default_track": 59.0,
  "exact_match_for_task1386_anli_r2_entailment_default_track": 33.0,
  "rouge1_for_task1386_anli_r2_entailment_default_track": 33.0,
  "rougeL_for_task1386_anli_r2_entailment_default_track": 33.0,
  "exact_match_for_task1152_bard_analogical_reasoning_causation_default_track": 21.0,
  "rouge1_for_task1152_bard_analogical_reasoning_causation_default_track": 21.0,
  "rougeL_for_task1152_bard_analogical_reasoning_causation_default_track": 21.0,
  "exact_match_for_task290_tellmewhy_question_answerability_default_track": 48.0,
  "rouge1_for_task290_tellmewhy_question_answerability_default_track": 82.6667,
  "rougeL_for_task290_tellmewhy_question_answerability_default_track": 82.6667,
  "exact_match_for_task304_numeric_fused_head_resolution_default_track": 7.0,
  "rouge1_for_task304_numeric_fused_head_resolution_default_track": 27.8557,
  "rougeL_for_task304_numeric_fused_head_resolution_default_track": 27.8557,
  "exact_match_for_task760_msr_sqa_long_text_generation_default_track": 0.0,
  "rouge1_for_task760_msr_sqa_long_text_generation_default_track": 14.9793,
  "rougeL_for_task760_msr_sqa_long_text_generation_default_track": 12.1497,
  "exact_match_for_task035_winogrande_question_modification_person_default_track": 15.0,
  "rouge1_for_task035_winogrande_question_modification_person_default_track": 89.3563,
  "rougeL_for_task035_winogrande_question_modification_person_default_track": 86.1491,
  "exact_match_for_task569_recipe_nlg_text_generation_default_track": 8.0,
  "rouge1_for_task569_recipe_nlg_text_generation_default_track": 44.4972,
  "rougeL_for_task569_recipe_nlg_text_generation_default_track": 40.951,
  "exact_match_for_task391_causal_relationship_default_track": 52.0,
  "rouge1_for_task391_causal_relationship_default_track": 84.0,
  "rougeL_for_task391_causal_relationship_default_track": 84.0,
  "exact_match_for_task891_gap_coreference_resolution_default_track": 22.0,
  "rouge1_for_task891_gap_coreference_resolution_default_track": 23.8667,
  "rougeL_for_task891_gap_coreference_resolution_default_track": 23.8667,
  "exact_match_for_task1586_scifact_title_generation_default_track": 0.0,
  "rouge1_for_task1586_scifact_title_generation_default_track": 31.8002,
  "rougeL_for_task1586_scifact_title_generation_default_track": 25.4835,
  "exact_match_for_task602_wikitext-103_answer_generation_default_track": 5.2632,
  "rouge1_for_task602_wikitext-103_answer_generation_default_track": 14.7149,
  "rougeL_for_task602_wikitext-103_answer_generation_default_track": 14.4518,
  "exact_match_for_task1195_disflqa_disfluent_to_fluent_conversion_default_track": 12.0,
  "rouge1_for_task1195_disflqa_disfluent_to_fluent_conversion_default_track": 80.638,
  "rougeL_for_task1195_disflqa_disfluent_to_fluent_conversion_default_track": 78.356,
  "exact_match_for_task1409_dart_text_generation_default_track": 2.0,
  "rouge1_for_task1409_dart_text_generation_default_track": 65.5329,
  "rougeL_for_task1409_dart_text_generation_default_track": 52.2379,
  "exact_match_for_task033_winogrande_answer_generation_default_track": 36.0,
  "rouge1_for_task033_winogrande_answer_generation_default_track": 37.3333,
  "rougeL_for_task033_winogrande_answer_generation_default_track": 37.3333,
  "exact_match_for_task1407_dart_question_generation_default_track": 0.0,
  "rouge1_for_task1407_dart_question_generation_default_track": 35.7993,
  "rougeL_for_task1407_dart_question_generation_default_track": 27.4381,
  "exact_match_for_task402_grailqa_paraphrase_generation_default_track": 0.0,
  "rouge1_for_task402_grailqa_paraphrase_generation_default_track": 80.4602,
  "rougeL_for_task402_grailqa_paraphrase_generation_default_track": 64.418,
  "exact_match_for_task201_mnli_neutral_classification_default_track": 25.0,
  "rouge1_for_task201_mnli_neutral_classification_default_track": 25.0,
  "rougeL_for_task201_mnli_neutral_classification_default_track": 25.0,
  "exact_match_for_task520_aquamuse_answer_given_in_passage_default_track": 86.0,
  "rouge1_for_task520_aquamuse_answer_given_in_passage_default_track": 86.0,
  "rougeL_for_task520_aquamuse_answer_given_in_passage_default_track": 86.0,
  "exact_match_for_task892_gap_reverse_coreference_resolution_default_track": 38.0,
  "rouge1_for_task892_gap_reverse_coreference_resolution_default_track": 38.0,
  "rougeL_for_task892_gap_reverse_coreference_resolution_default_track": 38.0,
  "exact_match_for_task828_copa_commonsense_cause_effect_default_track": 50.0,
  "rouge1_for_task828_copa_commonsense_cause_effect_default_track": 50.0,
  "rougeL_for_task828_copa_commonsense_cause_effect_default_track": 50.0,
  "exact_match_for_task769_qed_summarization_default_track": 72.0,
  "rouge1_for_task769_qed_summarization_default_track": 88.525,
  "rougeL_for_task769_qed_summarization_default_track": 88.525,
  "exact_match_for_task1155_bard_analogical_reasoning_trash_or_treasure_default_track": 72.0,
  "rouge1_for_task1155_bard_analogical_reasoning_trash_or_treasure_default_track": 72.6667,
  "rougeL_for_task1155_bard_analogical_reasoning_trash_or_treasure_default_track": 72.6667,
  "exact_match_for_task1385_anli_r1_entailment_default_track": 33.0,
  "rouge1_for_task1385_anli_r1_entailment_default_track": 33.0,
  "rougeL_for_task1385_anli_r1_entailment_default_track": 33.0,
  "exact_match_for_task1531_daily_dialog_type_classification_default_track": 32.0,
  "rouge1_for_task1531_daily_dialog_type_classification_default_track": 32.0,
  "rougeL_for_task1531_daily_dialog_type_classification_default_track": 32.0,
  "exact_match_for_task1516_imppres_naturallanguageinference_default_track": 47.0,
  "rouge1_for_task1516_imppres_naturallanguageinference_default_track": 47.0,
  "rougeL_for_task1516_imppres_naturallanguageinference_default_track": 47.0,
  "exact_match_for_task1394_meta_woz_task_classification_default_track": 33.0,
  "rouge1_for_task1394_meta_woz_task_classification_default_track": 36.1667,
  "rougeL_for_task1394_meta_woz_task_classification_default_track": 36.1667,
  "exact_match_for_task401_numeric_fused_head_reference_default_track": 39.0,
  "rouge1_for_task401_numeric_fused_head_reference_default_track": 50.5,
  "rougeL_for_task401_numeric_fused_head_reference_default_track": 50.5,
  "exact_match_for_task1598_nyc_long_text_generation_default_track": 0.0,
  "rouge1_for_task1598_nyc_long_text_generation_default_track": 58.489,
  "rougeL_for_task1598_nyc_long_text_generation_default_track": 42.8068,
  "exact_match_for_task1615_sick_tclassify_b_relation_a_default_track": 47.0,
  "rouge1_for_task1615_sick_tclassify_b_relation_a_default_track": 82.3333,
  "rougeL_for_task1615_sick_tclassify_b_relation_a_default_track": 82.3333,
  "exact_match_for_task970_sherliic_causal_relationship_default_track": 50.0,
  "rouge1_for_task970_sherliic_causal_relationship_default_track": 50.0,
  "rougeL_for_task970_sherliic_causal_relationship_default_track": 50.0,
  "exact_match_for_task1390_wscfixed_coreference_default_track": 50.0,
  "rouge1_for_task1390_wscfixed_coreference_default_track": 50.0,
  "rougeL_for_task1390_wscfixed_coreference_default_track": 50.0,
  "exact_match_for_task199_mnli_classification_default_track": 48.0,
  "rouge1_for_task199_mnli_classification_default_track": 48.0,
  "rougeL_for_task199_mnli_classification_default_track": 48.0,
  "exact_match_for_task034_winogrande_question_modification_object_default_track": 9.0,
  "rouge1_for_task034_winogrande_question_modification_object_default_track": 90.5043,
  "rougeL_for_task034_winogrande_question_modification_object_default_track": 86.8784,
  "exact_match_for_task133_winowhy_reason_plausibility_detection_default_track": 51.0,
  "rouge1_for_task133_winowhy_reason_plausibility_detection_default_track": 51.0,
  "rougeL_for_task133_winowhy_reason_plausibility_detection_default_track": 51.0,
  "exact_match_for_task226_english_language_answer_relevance_classification_default_track": 51.0,
  "rouge1_for_task226_english_language_answer_relevance_classification_default_track": 51.0,
  "rougeL_for_task226_english_language_answer_relevance_classification_default_track": 51.0,
  "exact_match_for_task510_reddit_tifu_title_summarization_default_track": 3.0,
  "rouge1_for_task510_reddit_tifu_title_summarization_default_track": 43.4785,
  "rougeL_for_task510_reddit_tifu_title_summarization_default_track": 43.0285,
  "exact_match_for_task935_defeasible_nli_atomic_classification_default_track": 62.0,
  "rouge1_for_task935_defeasible_nli_atomic_classification_default_track": 62.0,
  "rougeL_for_task935_defeasible_nli_atomic_classification_default_track": 62.0,
  "exact_match_for_task349_squad2.0_answerable_unanswerable_question_classification_default_track": 68.0,
  "rouge1_for_task349_squad2.0_answerable_unanswerable_question_classification_default_track": 68.0,
  "rougeL_for_task349_squad2.0_answerable_unanswerable_question_classification_default_track": 68.0,
  "exact_match_for_task1157_bard_analogical_reasoning_rooms_for_containers_default_track": 44.0,
  "rouge1_for_task1157_bard_analogical_reasoning_rooms_for_containers_default_track": 46.6667,
  "rougeL_for_task1157_bard_analogical_reasoning_rooms_for_containers_default_track": 46.6667,
  "exact_match_for_task937_defeasible_nli_social_classification_default_track": 54.0,
  "rouge1_for_task937_defeasible_nli_social_classification_default_track": 54.0,
  "rougeL_for_task937_defeasible_nli_social_classification_default_track": 54.0,
  "exact_match_for_task743_eurlex_summarization_default_track": 0.0,
  "rouge1_for_task743_eurlex_summarization_default_track": 30.5227,
  "rougeL_for_task743_eurlex_summarization_default_track": 23.8321,
  "exact_match_for_task1388_cb_entailment_default_track": 42.0,
  "rouge1_for_task1388_cb_entailment_default_track": 42.0,
  "rougeL_for_task1388_cb_entailment_default_track": 42.0,
  "exact_match_for_task671_ambigqa_text_generation_default_track": 0.0,
  "rouge1_for_task671_ambigqa_text_generation_default_track": 63.2962,
  "rougeL_for_task671_ambigqa_text_generation_default_track": 62.0084,
  "exact_match_for_task121_zest_text_modification_default_track": 0.0,
  "rouge1_for_task121_zest_text_modification_default_track": 53.4626,
  "rougeL_for_task121_zest_text_modification_default_track": 48.2788,
  "exact_match_for_task1345_glue_qqp_question_paraprashing_default_track": 0.0,
  "rouge1_for_task1345_glue_qqp_question_paraprashing_default_track": 41.5173,
  "rougeL_for_task1345_glue_qqp_question_paraprashing_default_track": 38.4136,
  "exact_match_for_task330_gap_answer_generation_default_track": 55.0,
  "rouge1_for_task330_gap_answer_generation_default_track": 61.2333,
  "rougeL_for_task330_gap_answer_generation_default_track": 61.2333,
  "exact_match_for_task1342_amazon_us_reviews_title_default_track": 2.0,
  "rouge1_for_task1342_amazon_us_reviews_title_default_track": 13.7369,
  "rougeL_for_task1342_amazon_us_reviews_title_default_track": 13.2159,
  "exact_match_for_task329_gap_classification_default_track": 35.0,
  "rouge1_for_task329_gap_classification_default_track": 35.9357,
  "rougeL_for_task329_gap_classification_default_track": 35.9357,
  "exact_match_for_task281_points_of_correspondence_default_track": 0.0,
  "rouge1_for_task281_points_of_correspondence_default_track": 24.6052,
  "rougeL_for_task281_points_of_correspondence_default_track": 23.9634,
  "exact_match_for_task036_qasc_topic_word_to_generate_related_fact_default_track": 3.0,
  "rouge1_for_task036_qasc_topic_word_to_generate_related_fact_default_track": 48.5438,
  "rougeL_for_task036_qasc_topic_word_to_generate_related_fact_default_track": 46.4958,
  "exact_match_for_task1554_scitail_classification_default_track": 66.0,
  "rouge1_for_task1554_scitail_classification_default_track": 66.0,
  "rougeL_for_task1554_scitail_classification_default_track": 66.0,
  "exact_match_for_task050_multirc_answerability_default_track": 69.0,
  "rouge1_for_task050_multirc_answerability_default_track": 69.0,
  "rougeL_for_task050_multirc_answerability_default_track": 69.0,
  "exact_match_for_task362_spolin_yesand_prompt_response_sub_classification_default_track": 50.0,
  "rouge1_for_task362_spolin_yesand_prompt_response_sub_classification_default_track": 75.0,
  "rougeL_for_task362_spolin_yesand_prompt_response_sub_classification_default_track": 75.0,
  "exact_match_for_task1557_jfleg_answer_generation_default_track": 19.0,
  "rouge1_for_task1557_jfleg_answer_generation_default_track": 89.0384,
  "rougeL_for_task1557_jfleg_answer_generation_default_track": 87.889,
  "exact_match_for_task249_enhanced_wsc_pronoun_disambiguation_default_track": 3.0,
  "rouge1_for_task249_enhanced_wsc_pronoun_disambiguation_default_track": 9.8714,
  "rougeL_for_task249_enhanced_wsc_pronoun_disambiguation_default_track": 9.8714,
  "exact_match_for_task957_e2e_nlg_text_generation_generate_default_track": 0.0,
  "rouge1_for_task957_e2e_nlg_text_generation_generate_default_track": 63.4064,
  "rougeL_for_task957_e2e_nlg_text_generation_generate_default_track": 46.621,
  "exact_match_for_task418_persent_title_generation_default_track": 1.0,
  "rouge1_for_task418_persent_title_generation_default_track": 25.8623,
  "rougeL_for_task418_persent_title_generation_default_track": 22.7409,
  "exact_match_for_task614_glucose_cause_event_detection_default_track": 0.0,
  "rouge1_for_task614_glucose_cause_event_detection_default_track": 54.7009,
  "rougeL_for_task614_glucose_cause_event_detection_default_track": 48.0299,
  "exact_match_for_task677_ollie_sentence_answer_generation_default_track": 0.0,
  "rouge1_for_task677_ollie_sentence_answer_generation_default_track": 39.7514,
  "rougeL_for_task677_ollie_sentence_answer_generation_default_track": 31.511,
  "exact_match_for_task220_rocstories_title_classification_default_track": 84.0,
  "rouge1_for_task220_rocstories_title_classification_default_track": 84.0,
  "rougeL_for_task220_rocstories_title_classification_default_track": 84.0,
  "exact_match_for_task1631_openpi_answer_generation_default_track": 0.0,
  "rouge1_for_task1631_openpi_answer_generation_default_track": 49.9744,
  "rougeL_for_task1631_openpi_answer_generation_default_track": 45.9368,
  "exact_match_for_task232_iirc_link_number_classification_default_track": 50.0,
  "rouge1_for_task232_iirc_link_number_classification_default_track": 50.0,
  "rougeL_for_task232_iirc_link_number_classification_default_track": 50.0,
  "exact_match_for_task1391_winogrande_easy_answer_generation_default_track": 50.0,
  "rouge1_for_task1391_winogrande_easy_answer_generation_default_track": 50.0,
  "rougeL_for_task1391_winogrande_easy_answer_generation_default_track": 50.0,
  "exact_match_for_task1358_xlsum_title_generation_default_track": 0.0,
  "rouge1_for_task1358_xlsum_title_generation_default_track": 40.4815,
  "rougeL_for_task1358_xlsum_title_generation_default_track": 34.9777,
  "exact_match_for_task1533_daily_dialog_formal_classification_default_track": 36.0,
  "rouge1_for_task1533_daily_dialog_formal_classification_default_track": 36.0,
  "rougeL_for_task1533_daily_dialog_formal_classification_default_track": 36.0,
  "exact_match_for_task1156_bard_analogical_reasoning_tools_default_track": 64.0,
  "rouge1_for_task1156_bard_analogical_reasoning_tools_default_track": 65.3333,
  "rougeL_for_task1156_bard_analogical_reasoning_tools_default_track": 65.3333,
  "exact_match_for_task1659_title_generation_default_track": 35.0,
  "rouge1_for_task1659_title_generation_default_track": 53.0443,
  "rougeL_for_task1659_title_generation_default_track": 49.4269,
  "exact_match_for_task1624_disfl_qa_question_yesno_classification_default_track": 46.0,
  "rouge1_for_task1624_disfl_qa_question_yesno_classification_default_track": 46.0,
  "rougeL_for_task1624_disfl_qa_question_yesno_classification_default_track": 46.0,
  "exact_match_for_task1158_bard_analogical_reasoning_manipulating_items_default_track": 40.0,
  "rouge1_for_task1158_bard_analogical_reasoning_manipulating_items_default_track": 46.6667,
  "rougeL_for_task1158_bard_analogical_reasoning_manipulating_items_default_track": 46.6667,
  "exact_match_for_task827_copa_commonsense_reasoning_default_track": 57.0,
  "rouge1_for_task827_copa_commonsense_reasoning_default_track": 57.0,
  "rougeL_for_task827_copa_commonsense_reasoning_default_track": 57.0,
  "exact_match_for_task1153_bard_analogical_reasoning_affordance_default_track": 26.0,
  "rouge1_for_task1153_bard_analogical_reasoning_affordance_default_track": 28.0667,
  "rougeL_for_task1153_bard_analogical_reasoning_affordance_default_track": 28.0667,
  "exact_match_for_task393_plausible_result_generation_default_track": 0.0,
  "rouge1_for_task393_plausible_result_generation_default_track": 37.459,
  "rougeL_for_task393_plausible_result_generation_default_track": 34.811,
  "exact_match_for_task879_schema_guided_dstc8_classification_default_track": 63.0,
  "rouge1_for_task879_schema_guided_dstc8_classification_default_track": 63.0,
  "rougeL_for_task879_schema_guided_dstc8_classification_default_track": 63.0,
  "exact_match_for_task613_politifact_text_generation_default_track": 29.0,
  "rouge1_for_task613_politifact_text_generation_default_track": 46.9667,
  "rougeL_for_task613_politifact_text_generation_default_track": 46.9667,
  "exact_match_for_task219_rocstories_title_answer_generation_default_track": 3.0,
  "rouge1_for_task219_rocstories_title_answer_generation_default_track": 19.1452,
  "rougeL_for_task219_rocstories_title_answer_generation_default_track": 19.1452,
  "exact_match_for_task190_snli_classification_default_track": 27.0,
  "rouge1_for_task190_snli_classification_default_track": 27.0,
  "rougeL_for_task190_snli_classification_default_track": 27.0,
  "exact_match_for_task200_mnli_entailment_classification_default_track": 60.0,
  "rouge1_for_task200_mnli_entailment_classification_default_track": 60.0,
  "rougeL_for_task200_mnli_entailment_classification_default_track": 60.0,
  "exact_match_for_task1534_daily_dialog_question_classification_default_track": 48.0,
  "rouge1_for_task1534_daily_dialog_question_classification_default_track": 48.0,
  "rougeL_for_task1534_daily_dialog_question_classification_default_track": 48.0,
  "exact_match_for_task1540_parsed_pdfs_summarization_default_track": 0.0,
  "rouge1_for_task1540_parsed_pdfs_summarization_default_track": 29.0272,
  "rougeL_for_task1540_parsed_pdfs_summarization_default_track": 25.9364,
  "exact_match_for_task442_com_qa_paraphrase_question_generation_default_track": 7.0,
  "rouge1_for_task442_com_qa_paraphrase_question_generation_default_track": 75.235,
  "rougeL_for_task442_com_qa_paraphrase_question_generation_default_track": 71.5862,
  "exact_match_for_task392_inverse_causal_relationship_default_track": 53.0,
  "rouge1_for_task392_inverse_causal_relationship_default_track": 84.3333,
  "rougeL_for_task392_inverse_causal_relationship_default_track": 84.3333,
  "exact_match_for_task1562_zest_text_modification_default_track": 1.0,
  "rouge1_for_task1562_zest_text_modification_default_track": 58.2549,
  "rougeL_for_task1562_zest_text_modification_default_track": 50.4989,
  "exact_match_for_task640_esnli_classification_default_track": 62.0,
  "rouge1_for_task640_esnli_classification_default_track": 62.0,
  "rougeL_for_task640_esnli_classification_default_track": 62.0,
  "exact_match_for_task1622_disfl_qa_text_modication_default_track": 11.0,
  "rouge1_for_task1622_disfl_qa_text_modication_default_track": 81.219,
  "rougeL_for_task1622_disfl_qa_text_modication_default_track": 79.2973,
  "exact_match_for_task623_ohsumed_yes_no_answer_generation_default_track": 36.0,
  "rouge1_for_task623_ohsumed_yes_no_answer_generation_default_track": 36.5714,
  "rougeL_for_task623_ohsumed_yes_no_answer_generation_default_track": 36.5714,
  "exact_match_for_task020_mctaco_span_based_question_default_track": 48.0,
  "rouge1_for_task020_mctaco_span_based_question_default_track": 48.0,
  "rougeL_for_task020_mctaco_span_based_question_default_track": 48.0,
  "exact_match_for_task642_esnli_classification_default_track": 47.0,
  "rouge1_for_task642_esnli_classification_default_track": 47.0,
  "rougeL_for_task642_esnli_classification_default_track": 47.0,
  "exact_match_for_task102_commongen_sentence_generation_default_track": 0.0,
  "rouge1_for_task102_commongen_sentence_generation_default_track": 67.5473,
  "rougeL_for_task102_commongen_sentence_generation_default_track": 62.788
}